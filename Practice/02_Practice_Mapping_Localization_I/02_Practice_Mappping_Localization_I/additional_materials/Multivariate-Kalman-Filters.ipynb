{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](./table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Kalman Filters\n",
    "\n",
    "Filtering Multiple Random Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#format the book\n",
    "import book_format\n",
    "book_format.set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Equations of Motion\n",
    "\n",
    "\n",
    "$$x = vt + x_0$$\n",
    "\n",
    "\n",
    "$$x = \\frac{1}{2}at^2 + v_0t + x_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Predict**</u>\n",
    "\n",
    "$\\begin{array}{|l|l|l|}\n",
    "\\hline\n",
    "\\text{Univariate} & \\text{Univariate} & \\text{Multivariate}\\\\\n",
    "& \\text{(Kalman form)} & \\\\\n",
    "\\hline\n",
    "\\bar \\mu = \\mu + \\mu_{f_x} & \\bar x = x + dx & \\bar{\\mathbf x} = \\mathbf{Fx} + \\mathbf{Bu}\\\\\n",
    "\\bar\\sigma^2 = \\sigma_x^2 + \\sigma_{f_x}^2 & \\bar P = P + Q & \\bar{\\mathbf P} = \\mathbf{FPF}^\\mathsf T + \\mathbf Q \\\\\n",
    "\\hline\n",
    "\\end{array}$\n",
    "\n",
    "Without worrying about the specifics of the linear algebra, we can see that:\n",
    "\n",
    "$\\mathbf x,\\, \\mathbf P$ are the state mean and covariance. They correspond to $x$ and $\\sigma^2$.\n",
    "\n",
    "$\\mathbf F$ is the *state transition function*. When multiplied by $\\bf x$ it computes the prior. \n",
    "\n",
    "$\\mathbf Q$ is the process covariance. It corresponds to $\\sigma^2_{f_x}$.\n",
    "\n",
    "$\\mathbf B$ and $\\mathbf u$ are new to us. They let us model control inputs to the system.\n",
    "\n",
    "<u>**Update**</u>\n",
    "\n",
    "$\\begin{array}{|l|l|l|}\n",
    "\\hline\n",
    "\\text{Univariate} & \\text{Univariate} & \\text{Multivariate}\\\\\n",
    "& \\text{(Kalman form)} & \\\\\n",
    "\\hline\n",
    "& y = z - \\bar x & \\mathbf y = \\mathbf z - \\mathbf{H\\bar x} \\\\\n",
    "& K = \\frac{\\bar P}{\\bar P+R}&\n",
    "\\mathbf K = \\mathbf{\\bar{P}H}^\\mathsf T (\\mathbf{H\\bar{P}H}^\\mathsf T + \\mathbf R)^{-1} \\\\\n",
    "\\mu=\\frac{\\bar\\sigma^2\\, \\mu_z + \\sigma_z^2 \\, \\bar\\mu} {\\bar\\sigma^2 + \\sigma_z^2} & x = \\bar x + Ky & \\mathbf x = \\bar{\\mathbf x} + \\mathbf{Ky} \\\\\n",
    "\\sigma^2 = \\frac{\\sigma_1^2\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2} & P = (1-K)\\bar P &\n",
    "\\mathbf P = (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar{P}} \\\\\n",
    "\\hline\n",
    "\\end{array}$\n",
    "\n",
    "$\\mathbf H$ is the measurement function.\n",
    "\n",
    "$\\mathbf z,\\, \\mathbf R$ are the measurement mean and noise covariance. They correspond to $z$ and $\\sigma_z^2$ in the univariate filter (I've substituted $\\mu$ with $x$ for the univariate equations to make the notation as similar as possible).\n",
    "\n",
    "$\\mathbf y$ and $\\mathbf K$ are the residual and Kalman gain. \n",
    "\n",
    "The details will be different than the univariate filter because these are vectors and matrices, but the concepts are exactly the same: \n",
    "\n",
    "-  Use a Gaussian to represent our estimate of the state and error\n",
    "-  Use a Gaussian to represent the measurement and its error\n",
    "-  Use a Gaussian to represent the process model\n",
    "-  Use the process model to predict the next state (the prior)\n",
    "-  Form an estimate part way between the measurement and the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filterpy provides the necessary tools.\n",
    "\n",
    "```python\n",
    "from filterpy.kalman import predict, update\n",
    "```\n",
    "\n",
    "https://github.com/rlabbe/filterpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking a Dog\n",
    "\n",
    "Let's go back to our tried and true problem of tracking a dog. This time we will include the fundamental insight of the previous chapter and use *hidden variables* to improve our estimates. I could start with the math, but instead let's implement a filter, learning as we go. On the surface the math is different and perhaps more complicated than the previous chapters, but the ideas are all the same - we are just multiplying and adding Gaussians.\n",
    "\n",
    "We start by writing a simulation for the dog. The simulation will run for `count` steps, moving the dog forward approximately 1 meter for each step. At each step the velocity will vary according to the process variance `process_var`. After updating the position we compute a measurement with an assumed sensor variance of `z_var`. The function returns an NumPy array of the positions and another of the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "def compute_dog_data(z_var, process_var, count=1, dt=1.):\n",
    "    \"returns track, measurements 1D ndarrays\"\n",
    "    x, vel = 0., 1.\n",
    "    z_std = math.sqrt(z_var) \n",
    "    p_std = math.sqrt(process_var)\n",
    "    xs, zs = [], []\n",
    "    for _ in range(count):\n",
    "        v = vel + (randn() * p_std)\n",
    "        x += v*dt        \n",
    "        xs.append(x)\n",
    "        zs.append(x + randn() * z_std)        \n",
    "    return np.array(xs), np.array(zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Step\n",
    "\n",
    "For the prediction we need to design the state and covariance, the process model and the process noise, and optionally the control input. We'll take them in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design State Variable\n",
    "\n",
    "We previously tracked a dog in one dimension by using a Gaussian. The mean $(\\mu)$ represented the most likely position, and the variance ($\\sigma^2$) represented the probability distribution of the position. The position is the *state* of the system, and we call $\\mu$ the *state variable*. \n",
    "\n",
    "In this problem we will be tracking both the position and velocity of the dog. This requires us to use a multivariate Gaussian represented with the state vector $\\mathbf x$ and its corresponding covariance matrix $\\mathbf P$. \n",
    "\n",
    "State variables can either be *observed variables* - directly measured by a sensor, or *hidden variables* - inferred from the observed variables. For our dog tracking problem the sensor only reads position, so position is observed and velocity is hidden. We will learn how to track hidden variables soon.\n",
    "\n",
    "It is important to understand that tracking position and velocity is a design choice with implications and assumptions that we are not yet prepared to explore. For example, we could also track acceleration, or even jerk. For now, recall that in the last chapter we showed that including velocity in the covariance matrix resulted in much smaller variances in position. We will learn how the Kalman filter computes estimates for hidden variables later in this chapter. \n",
    "\n",
    "In the univariate chapter we represented the dog's position with a scalar value (e.g. $\\mu=3.27$). In the last chapter we learned to use a multivariate Gaussian for multiple variables. For example, if we wanted to specify a position of 10.0 m and a velocity of 4.5 m/s, we would write:\n",
    "\n",
    "$$\\mu = \\begin{bmatrix}10.0\\\\4.5\\end{bmatrix}$$\n",
    "\n",
    "The Kalman filter is implemented using linear algebra. We use an $n\\times 1$ matrix (called a *vector*) to store  $n$ state variables. For the dog tracking problem, we use $x$ to denote position, and the first derivative of $x$, $\\dot x$, for velocity. I use Newton's dot notation for derivatives; $\\dot x$ represents the first derivative of x with respect to t: $\\dot x = \\frac{dx}{dt}$. Kalman filter equations use $\\mathbf x$ for the state, so we define $\\mathbf x$ as:\n",
    "\n",
    "$$\\mathbf x =\\begin{bmatrix}x \\\\ \\dot x\\end{bmatrix}$$\n",
    "\n",
    "We use $\\mathbf x$ instead of $\\mu$, but recognize this is the mean of the multivariate Gaussian.\n",
    "\n",
    "Another way to write this is $\\mathbf x =\\begin{bmatrix}x & \\dot x\\end{bmatrix}^\\mathsf T$ because the transpose of a row vector is a column vector. This notation is easier to use in text because it takes less vertical space.\n",
    "\n",
    "$\\mathbf x$ and the position $x$ coincidentally have the same name. If we were tracking the dog in the y-axis we would write $\\mathbf x =\\begin{bmatrix}y & \\dot y\\end{bmatrix}^\\mathsf T$, not  $\\mathbf y =\\begin{bmatrix}y & \\dot y\\end{bmatrix}^\\mathsf T$. $\\mathbf x$ is the standard name for the state variable used in the Kalman filter literature and we will not vary it to give it a more meaningful name. This consistency in naming allows us to communicate with our peers.\n",
    "\n",
    "Let's code this. Initialization of `x` is as simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[10.0],\n",
    "              [4.5]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I often use the transpose in my code to turn a row matrix into a column vector, as I find it easier to type and read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[10., 4.5]]).T\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, NumPy recognizes 1D arrays as vectors, so I can simplify this line to use a 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10.0, 4.5])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the array elements have the same type, typically either `float` or `int`. If the list contains all `int`s then the created array will also have a data type of `int`, otherwise it will be `float`. I will often take advantage of this by only specifying one number as a floating point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1., 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "x = np.array([[10.0], [4.5]])\n",
    "\n",
    "# matrix multiply\n",
    "print(np.dot(A, x))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python 3.5+ we have the matrix multiplier @, where `np.dot(A, B) == A @ B`. It is somewhat less useful then you might realize because it requires both `A` and `B` to be arrays. It is entirely valid in the math in this book for some of these variables to be scalars, therefore the utility of `@` is often lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# alternative matrix multiply)\n",
    "print(A @ x)\n",
    "print()\n",
    "\n",
    "x = np.array([[10.0, 4.5]]).T\n",
    "print(A @ x)\n",
    "print()\n",
    "\n",
    "x = np.array([10.0, 4.5])\n",
    "print(A @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last returns a 1D array, but I have written the Kalman filter class to be able to handle this. In retrospect that might lead to confusion, but it does work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design State Covariance\n",
    "\n",
    "The other half of the state Gaussian is the covariance matrix $\\mathbf P$. In the univariate Kalman filter we specified an initial value for $\\sigma^2$, and then the filter took care of updating its value as measurements were added to the filter. The same thing happens in the multidimensional Kalman filter. We specify an initial value for $\\mathbf P$ and the filter updates it during each epoch.\n",
    "\n",
    "We need to set the variances to reasonable values. For example, we may choose $\\sigma_\\mathtt{pos}^2=500 m^2$ if we are quite uncertain about the initial position. Top speed for a dog is around 21 m/s, so in the absence of any other information about the velocity we can set $3\\sigma_\\mathtt{vel}=21$, or $\\sigma_\\mathtt{vel}^2=7^2=49$. \n",
    "\n",
    "In the last chapter we showed that the position and velocities are correlated. But how correlated are they for a dog? I have no idea. As we will see the filter computes this for us, so I initialize the covariances to zero. Of course, if you know the covariances you should use them.\n",
    "\n",
    "Recall that the diagonals of the covariance matrix contains the variance of each variable, and the off-diagonal elements contains the covariances. Thus we have:\n",
    "\n",
    "$$\n",
    "\\mathbf P = \\begin{bmatrix}500 & 0 \\\\ 0&49\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can use `numpy.diag`, which creates a diagonal matrix from the values for the diagonal. Recall from linear algebra that a diagonal matrix is one with zeros in the off-diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.diag([500., 49.])\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could have written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[500., 0.],\n",
    "              [0., 49.]])\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done. We've expressed the state of the filter as a multivariate Gaussian and implemented it in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Design the Process Model\n",
    "\n",
    "The next step is designing the *process model*. It is a mathematical model which describes the behavior of the system. The filter uses it to predict the state after a discrete time step. We do this with a set of equations that describe the dynamics of the system.\n",
    "\n",
    "In the univariate chapter we modeled the dog's motion with\n",
    "\n",
    "$$ x = v \\Delta t + x_0$$\n",
    "\n",
    "We implemented this as follows:\n",
    "\n",
    "```python\n",
    "def predict(pos, movement):\n",
    "    return gaussian(pos.mean + movement.mean, \n",
    "                    pos.var + movement.var)\n",
    "```\n",
    "\n",
    "We will do the same thing in this chapter, using multivariate Gaussians instead of univariate Gaussians. You might imagine this sort of implementation:\n",
    "\n",
    "$$ \\mathbf x = \\begin{bmatrix}5.4\\\\4.2\\end{bmatrix}, \\, \\, \n",
    "\\dot{\\mathbf x} =  \\begin{bmatrix}1.1\\\\0.\\end{bmatrix} \\\\\n",
    "\\mathbf x = \\dot{\\mathbf x}t + \\mathbf x$$\n",
    "\n",
    "But we need to generalize this. The Kalman filter equations work with any linear system, not just Newtonian ones. Maybe the system you are filtering is the plumbing system in a chemical plant, and the flow in a given pipe is determined by a linear combination of the settings of different valves. \n",
    "\n",
    "$$\\mathtt{pipe_1} = 0.134(\\mathtt{valve}_1) + 0.41(\\mathtt{valve}_2 - \\mathtt{valve}_3) + 1.34$$\n",
    "$$\\mathtt{pipe_2} = 0.210(\\mathtt{valve}_2) - 0.62(\\mathtt{valve}_1 - \\mathtt{valve}_5) + 1.86$$\n",
    "\n",
    "Linear algebra has a powerful way to express systems of equations. Take this system\n",
    "\n",
    "$$\\begin{cases}\n",
    "2x+3y=8\\\\4x-y=2\n",
    "\\end{cases}$$\n",
    "\n",
    "We can put this in matrix form by writing:\n",
    "\n",
    "$$\\begin{bmatrix}2& 3 \\\\ 4&-1\\end{bmatrix} \\begin{bmatrix}x\\\\y\\end{bmatrix} = \\begin{bmatrix}8\\\\2\\end{bmatrix}$$\n",
    "\n",
    "If you perform the [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication#General_definition_of_the_matrix_product) in this equation the result will be the two equations above. In linear algebra we would write this as $\\mathbf{Ax}=\\mathbf B$, where\n",
    "\n",
    "$$\\mathbf{A} = \\begin{bmatrix}2& 3 \\\\ 4&-1\\end{bmatrix},\\, \\mathbf x = \\begin{bmatrix}x\\\\y\\end{bmatrix}, \\mathbf B=\\begin{bmatrix}8\\\\2\\end{bmatrix}$$\n",
    "\n",
    "And then we can use the SciPy's `linalg` package to solve for $\\mathbf x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve\n",
    "A = np.array([[2, 3],[4, -1]])\n",
    "b = np.array([[8], [2]])\n",
    "x = solve(A, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the process model to perform the *innovation*, because the equations tell us what the next state will be given the current state. Kalman filters implement this using this linear equation, where $\\mathbf{\\bar x}$ is the *prior*, or predicted state:\n",
    "\n",
    "$$\\mathbf{\\bar x} = \\mathbf{Fx}$$\n",
    "\n",
    "which we can make explicit as\n",
    "\n",
    "$$\\begin{bmatrix} \\bar x \\\\ \\dot{\\bar x}\\end{bmatrix} = \\begin{bmatrix}? & ? \\\\? & ?\\end{bmatrix}\\begin{bmatrix}x\\\\\\dot x\\end{bmatrix}$$\n",
    "\n",
    "Our job as Kalman filters designers is to specify $\\mathbf F$ such that $\\bar{\\mathbf x}  = \\mathbf{Fx}$ performs the innovation (prediction) for our system. To do this we need one equation for each state variable. In our problem $\\mathbf x = \\begin{bmatrix}x & \\dot x\\end{bmatrix}^\\mathtt{T}$, so we need one equation to compute the position $x$ and another to compute the velocity $\\dot x$ . We already know the equation for the position innovation:\n",
    "\n",
    "$$\\bar x = x + \\dot x \\Delta t$$\n",
    "\n",
    "What is our equation for velocity? We have no predictive model for how our dog's velocity will change over time. In this case we assume that it remains constant between innovations. Of course this is not exactly true, but so long as the velocity doesn't change too much over each innovation you will see that the filter performs very well. So we say\n",
    "\n",
    "$$\\bar{\\dot x} = \\dot x$$\n",
    "\n",
    "This gives us the process model for our system \n",
    "\n",
    "$$\\begin{cases}\n",
    "\\begin{aligned}\n",
    "\\bar x &= x + \\dot x \\Delta t \\\\\n",
    "\\bar{\\dot x} &= \\dot x\n",
    "\\end{aligned}\n",
    "\\end{cases}$$\n",
    "\n",
    "This correctly has one equation for each variable in the state, isolated on the left hand side. We need to express this set of equations in the form $\\bar{\\mathbf x}  = \\mathbf{Fx}$. Rearranging terms makes it easier to see what to do.\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\begin{aligned}\n",
    "\\bar x &= 1x + &\\Delta t\\, \\dot x \\\\\n",
    "\\bar{\\dot x} &=0x + &1\\, \\dot x\n",
    "\\end{aligned}\n",
    "\\end{cases}$$\n",
    "\n",
    "We can rewrite this in matrix form as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix}\\bar x \\\\ \\bar{\\dot x}\\end{bmatrix} &= \\begin{bmatrix}1&\\Delta t  \\\\ 0&1\\end{bmatrix}  \\begin{bmatrix}x \\\\ \\dot x\\end{bmatrix}\\\\\n",
    "\\mathbf{\\bar x} &= \\mathbf{Fx}\n",
    "\\end{aligned}$$\n",
    "\n",
    "$\\mathbf F$ is called the *state transition function* or the *state transition matrix*. In later chapters it will be a true function, not a  matrix, so calling it a function is a bit more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "F = np.array([[1, dt],\n",
    "              [0, 1]])\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this! FilterPy has a `predict` method that performs the prediction by computing $\\mathbf{\\bar x} = \\mathbf{Fx}$. Let's call it and see what happens. We've set the position to 10.0 and the velocity to  4.5 meter/sec. We've defined `dt = 0.1`, which means the time step is 0.1 seconds, so we expect the new position to be 10.45 meters after the innovation. The velocity should be unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import predict\n",
    "\n",
    "x = np.array([10.0, 4.5])\n",
    "P = np.diag([500, 49])\n",
    "F = np.array([[1, dt], [0, 1]])\n",
    "\n",
    "# Q is the process noise\n",
    "x, P = predict(x=x, P=P, F=F, Q=0)\n",
    "print('x =', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked. If we call `predict()` several times in a row the value will be updated each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    x, P = predict(x=x, P=P, F=F, Q=0)\n",
    "    print('x =', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict()` computes both the mean and covariance of the innovation. This is the value of $\\mathbf P$ after five innovations (predictions), which we denote $\\mathbf{\\bar P}$ in the Kalman filter equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the diagonals shows us that the position variance got larger. We've performed five prediction steps with no measurements, and our uncertainty grew. The off-diagonal elements became non-zero - the Kalman filter detected a correlation between position and velocity! The variance of the velocity did not change.\n",
    "\n",
    "Here I plot the covariance before and after the prediction. The initial value is in solid red, and the prior (prediction) is in dashed black. I've altered the covariance and time step to better illustrate the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.stats import plot_covariance_ellipse\n",
    "\n",
    "dt = 0.3\n",
    "F = np.array([[1, dt], [0, 1]])\n",
    "x = np.array([10.0, 4.5])\n",
    "P = np.diag([500, 500])\n",
    "plot_covariance_ellipse(x, P, edgecolor='r')\n",
    "x, P = predict(x, P, F, Q=0)\n",
    "plot_covariance_ellipse(x, P, edgecolor='k', ls='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the center of the ellipse shifted by a small amount (from 10 to 11.35) because the position changed. The ellipse also elongated, showing the correlation between position and velocity. How does the filter compute new values for $\\mathbf{\\bar P}$, and what is it based on? Note that I set the process noise `Q` to zero each time, so it is not due to me adding noise. It's a little to early to discuss this, but recall that in every filter so far the predict step entailed a loss of information. The same is true here. I will give you the details once we have covered a bit more ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Process Noise\n",
    "\n",
    "A quick review on *process noise*. A car is driving along the road with the cruise control on; it should travel at a constant speed. We model this with $\\bar x_k=\\dot x_k\\Delta t + x_{k-1}$. However, it is affected by a number of unknown factors. The cruise control cannot perfectly maintain a constant velocity. Winds affect the car, as do hills and potholes. Passengers roll down windows, changing the drag profile of the car. \n",
    "\n",
    "We can model this system with the differential equation\n",
    "\n",
    "$$\\dot{\\mathbf x} = f(\\mathbf x) + w$$\n",
    "\n",
    "where $f(\\mathbf x)$ models the state transition and $w$ is *white process noise*.\n",
    "\n",
    "We will learn how to go from a set of differential equations to the Kalman filter matrices in the **Kalman Filter Math** chapter. In this chapter we take advantage of the fact that Newton already derived the equations of motion for us. For now you just need to know that we account for the  noise in the system by adding a process noise covariance matrix $\\mathbf Q$ to the covariance $\\mathbf P$. We do not add anything to $\\mathbf x$ because the noise is *white* - which means that the mean of the noise will be 0. If the mean is 0, $\\mathbf x$ will not change.\n",
    "\n",
    "The univariate Kalman filter used `variance = variance + process_noise` to compute the variance for the variance of the prediction step. The multivariate Kalman filter does the same, essentially `P = P + Q`. I say 'essentially' because there are other terms unrelated to noise in the covariance equation that we will see later.\n",
    "\n",
    "Deriving the process noise matrix can be quite demanding, and we will put it off until the Kalman math chapter. For now know that $\\mathbf Q$ equals the expected value of the white noise $w$, computed as $\\mathbf Q = \\mathbb E[\\mathbf{ww}^\\mathsf T]$. In this chapter we will focus on building an intuitive understanding on how modifying this matrix alters the behavior of the filter.\n",
    "\n",
    "FilterPy provides functions which compute $\\mathbf Q$ for the kinematic problems of this chapter. `Q_discrete_white_noise` takes 3 parameters. `dim`, which specifies the dimension of the matrix, `dt`, which is the time step in seconds, and `var`, the variance in the noise. Briefly, it discretizes the noise over the given time period under assumptions that we will discuss later. This code computes $\\mathbf Q$ for white noise with a variance of 2.35 and a time step of 1 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from filterpy.common import Q_discrete_white_noise\n",
    "Q = Q_discrete_white_noise(dim=2, dt=1., var=2.35)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the Control Function\n",
    "\n",
    "The Kalman filter does not just filter data, it allows us to incorporate the control inputs of systems like robots and airplanes. Suppose we are controlling a robot. At each time step we would send steering and velocity signals to the robot based on its current position vs desired position. Kalman filter equations incorporate that knowledge into the filter equations, creating a predicted position based both on current velocity and control inputs to the drive motors. Remember, we *never* throw information away.\n",
    "\n",
    "For a linear system the effect of control inputs can be described as a set of linear equations, which we can express with linear algebra as\n",
    "\n",
    "$$\\Delta\\mathbf x = \\mathbf{Bu}$$\n",
    "\n",
    "Here $\\mathbf u$ is the *control input*, and $\\mathbf B$ is the *control input model* or *control function*. For example, $\\mathbf u$ might be a voltage controlling how fast the wheel's motor turns, and multiplying by $\\mathbf B$ yields $\\Delta[\\begin{smallmatrix}x\\\\\\dot x\\end{smallmatrix}]$. In other words, it must compute how much $\\mathbf x$ changes due to the control input.\n",
    "\n",
    "Therefore the complete Kalman filter equation for the prior mean is\n",
    "\n",
    "$$\\mathbf{\\bar x} = \\mathbf{Fx} + \\mathbf{Bu}$$\n",
    "\n",
    "and this is the equation that is computed when you call `KalmanFilter.predict()`.\n",
    "\n",
    "Your dog may be trained to respond to voice commands. All available evidence suggests that my dog has no control inputs whatsoever, so I set $\\mathbf B$ to zero. In Python we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 0.  # my dog doesn't listen to me!\n",
    "u = 0\n",
    "x, P = predict(x, P, F, Q, B, u)\n",
    "print('x =', x)\n",
    "print('P =', P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting $\\mathbf B$ and $\\mathbf u$ to zero is not necessary since `predict` uses 0 for their default value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x, P, F, Q)[0] == predict(x, P, F, Q, B, u)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x, P, F, Q)[1] == predict(x, P, F, Q, B, u)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Prediction: Summary\n",
    "\n",
    "Your job as a designer is to specify the matrices for\n",
    "\n",
    "* $\\mathbf x$, $\\mathbf P$: the state and covariance\n",
    "* $\\mathbf F$,  $\\mathbf Q$: the process model and noise covariance\n",
    "* $\\mathbf{B,u}$: Optionally, the control input and function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Step\n",
    "\n",
    "Now we can implement the update step of the filter. You only have to supply two more matrices, and they are easy to understand. \n",
    "\n",
    "### Design the Measurement Function\n",
    "\n",
    "The Kalman filter computes the update step in what we call *measurement space*. We mostly ignored this issue in the univariate chapter because of the complication it adds. We tracked our dog's position using a sensor that reported his position. Computing the *residual* was easy - subtract the filter's predicted position from the measurement:\n",
    "\n",
    "$$ \\mathtt{residual} = \\mathtt{measured\\, \\, position} - \\mathtt{predicted\\, \\, position}$$\n",
    "\n",
    "We need to compute the residual because we scale it by the Kalman gain to get the new estimate.\n",
    "\n",
    "What would happen if we were trying to track temperature using a thermometer that outputs a voltage corresponding to the temperature reading? The equation for the residual computation would be meaningless; you can't subtract a temperature from a voltage.\n",
    "\n",
    "$$ \\mathtt{residual} = \\mathtt{voltage} - \\mathtt{temperature}\\;\\;\\;(NONSENSE!)$$\n",
    "\n",
    "\n",
    "We need to convert the temperature into a voltage so we can perform the subtraction. For the thermometer we might write:\n",
    "\n",
    "```python\n",
    "CELSIUS_TO_VOLTS = 0.21475\n",
    "residual = voltage - (CELSIUS_TO_VOLTS * predicted_temperature)\n",
    "```\n",
    "    \n",
    "The Kalman filter generalizes this problem by having you supply a *measurement function* that converts a state into a measurement. \n",
    "\n",
    "Why are we working in measurement space? Why not work in state space by converting the voltage into a temperature, allowing the residual to be a difference in temperature?\n",
    "\n",
    "We cannot do that because most measurements are not *invertible*. The state for the tracking problem contains the hidden variable $\\dot x$. There is no way to convert a measurement of position into a state containing velocity. On the other hand, it is trivial to convert a state containing position and velocity into a equivalent \"measurement\" containing only position. We have to work in measurement space to make the computation of the residual possible.\n",
    "\n",
    "Both the measurement $\\mathbf z$ and state $\\mathbf x$ are vectors so we need to use a matrix to perform the conversion. The Kalman filter equation that performs this step is:\n",
    "\n",
    "$$\\mathbf y = \\mathbf z - \\mathbf{H \\bar x}$$\n",
    "\n",
    "where $\\mathbf y$ is the residual, $\\mathbf{\\bar x}$ is the prior, $\\mathbf z$ is the measurement, and $\\mathbf H$ is the measurement function. So we take the prior, convert it to a measurement by multiplying it with $\\mathbf H$, and subtract that from the measurement. This gives us the difference between our prediction and measurement in measurement space!\n",
    "<img src=\"./figs/residual_chart_with_h.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to design $\\mathbf H$ so that $\\mathbf{H\\bar x}$ yields a measurement. For this problem we have a sensor that measures position, so $\\mathbf z$ will be a one variable vector:\n",
    "\n",
    "$$\\mathbf z = \\begin{bmatrix}z\\end{bmatrix}$$\n",
    "\n",
    "The residual equation will have the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{y} &= \\mathbf z - \\mathbf{H\\bar x}  \\\\\n",
    "\\begin{bmatrix}y \\end{bmatrix} &= \\begin{bmatrix}z\\end{bmatrix} - \\begin{bmatrix}?&?\\end{bmatrix} \\begin{bmatrix}x \\\\ \\dot x\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\mathbf H$ has to be a 1x2 matrix for $\\mathbf{Hx}$ to be 1x1. Recall that multiplying matrices $m\\times n$ by $n\\times p$ yields a $m\\times p$ matrix.\n",
    "\n",
    "We will want to multiply the position $x$ by 1 to get the corresponding measurement of the position. We do not need to use velocity to find the corresponding measurement so we multiply  $\\dot x$ by 0.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\textbf{y} &= \\mathbf z - \\begin{bmatrix}1&0\\end{bmatrix} \\begin{bmatrix}x \\\\ \\dot x\\end{bmatrix} \\\\\n",
    "&= [z] - [x]\n",
    "\\end{aligned}$$\n",
    "\n",
    "And so, for our Kalman filter we set\n",
    "\n",
    "$$\\mathbf H=\\begin{bmatrix}1&0\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.array([[1., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have designed the majority of our Kalman filter. All that is left is to model the noise in the sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Design the Measurement\n",
    "\n",
    "The measurement is implemented with $\\mathbf z$, the measurement mean, and $\\mathbf R$, the measurement covariance. \n",
    "\n",
    "$\\mathbf z$ is easy. it contains the measurement(s) as a vector. We have only one measurement, so we have:\n",
    "\n",
    "$$\\mathbf z = \\begin{bmatrix}z\\end{bmatrix}$$\n",
    "\n",
    "If we have two sensors or measurements we'd have:\n",
    "\n",
    "$$\\mathbf z = \\begin{bmatrix}z_1 \\\\ z_2\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The *measurement noise matrix* models the noise in our sensors as a covariance matrix. In practice this can be difficult. A complicated system may have many sensors, the correlation between them might not be clear, and usually their noise is not a pure Gaussian. For example, a sensor might be biased to read high if the temperature is high, and so the noise is not distributed equally on both sides of the mean. We will learn to deal with these problems later.\n",
    "\n",
    "The Kalman filter equations uses a covariance matrix $\\mathbf R$ for the measurement noise. The matrix will have dimension $m{\\times}m$, where $m$ is the number of sensors. It is a covariance matrix to account for correlations between the sensors. We have only 1 sensor so R is:\n",
    "\n",
    "$$R = \\begin{bmatrix}\\sigma^2_z\\end{bmatrix}$$\n",
    "\n",
    "If $\\sigma^2_z$ is 5 meters squared we'd have $R = \\begin{bmatrix}5\\end{bmatrix}$. \n",
    "\n",
    "If we had two position sensors, the first with a variance of 5 m$^2$, the second with a variance of 3 m$^2$, we would write\n",
    "\n",
    "$$R = \\begin{bmatrix}5&0\\\\0&3\\end{bmatrix}$$\n",
    "\n",
    "We put the variances on the diagonal because this is a *covariance* matrix, where the variances lie on the diagonal, and the covariances, if any, lie in the off-diagonal elements. Here we assume there is no correlation in the noise between the two sensors, so the covariances are 0.\n",
    "\n",
    "For our problem we only have one sensor, so we can implement this as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([[5.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the update by calling `update`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import update\n",
    "z = 1.\n",
    "x, P = update(x, P, z, R, H)\n",
    "print('x =', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping track of all of these variables is burdensome, so FilterPy also implements the filter with the class `KalmanFilter`. I will use the class in the rest of this book, but I wanted you to see the procedural form of these functions since I know some of you are not fans of object oriented programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Kalman Filter\n",
    "\n",
    "I've given you all of the code for the filter, but now let's collect it in one place. First we construct a `KalmanFilter` object. We have to specify the number of variables in the state with the `dim_x` parameter, and the number of measurements with `dim_z`. We have two random variables in the state and one measurement, so we write:\n",
    "\n",
    "```python\n",
    "from filterpy.kalman import KalmanFilter\n",
    "dog_filter = KalmanFilter(dim_x=2, dim_z=1)\n",
    "```\n",
    "\n",
    "This creates an object with default values for all the Kalman filter matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import KalmanFilter\n",
    "dog_filter = KalmanFilter(dim_x=2, dim_z=1)\n",
    "print('x = ', dog_filter.x.T)\n",
    "print('R = ', dog_filter.R)\n",
    "print('Q = \\n', dog_filter.Q)\n",
    "# etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the filter's matrices and vectors with values valid for our problem. I've put this in a function to allow you to specify different initial values for `R`, `P`, and `Q` and put it in a helper function. We will be creating and running many of these filters, and this saves us a lot of headaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "\n",
    "def pos_vel_filter(x, P, R, Q=0., dt=1.0):\n",
    "    \"\"\" Returns a KalmanFilter which implements a\n",
    "    constant velocity model for a state [x dx].T\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KalmanFilter(dim_x=2, dim_z=1)\n",
    "    kf.x = np.array([x[0], x[1]]) # location and velocity\n",
    "    kf.F = np.array([[1., dt],\n",
    "                     [0.,  1.]])  # state transition matrix\n",
    "    kf.H = np.array([[1., 0]])    # Measurement function\n",
    "    kf.R *= R                     # measurement uncertainty\n",
    "    if np.isscalar(P):\n",
    "        kf.P *= P                 # covariance matrix \n",
    "    else:\n",
    "        kf.P[:] = P               # [:] makes deep copy\n",
    "    if np.isscalar(Q):\n",
    "        kf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=Q)\n",
    "    else:\n",
    "        kf.Q[:] = Q\n",
    "    return kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KalmanFilter` initializes `R`, `P`, and `Q` to the identity matrix, so `kf.P *= P` is one way to quickly assign all of the diagonal elements to the same scalar value. Now we create the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = .1\n",
    "x = np.array([0., 0.]) \n",
    "kf = pos_vel_filter(x, P=500, R=5, Q=0.1, dt=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the current values of all attributes of the filter by entering the variable on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left is to write the code to run the Kalman filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kf_book.mkf_internal import plot_track\n",
    "\n",
    "def run(x0=(0.,0.), P=500, R=0, Q=0, dt=1.0, \n",
    "        track=None, zs=None,\n",
    "        count=0, do_plot=True, **kwargs):\n",
    "    \"\"\"\n",
    "    track is the actual position of the dog, zs are the \n",
    "    corresponding measurements. \n",
    "    \"\"\"\n",
    "\n",
    "    # Simulate dog if no data provided. \n",
    "    if zs is None:\n",
    "        track, zs = compute_dog_data(R, Q, count)\n",
    "\n",
    "    # create the Kalman filter\n",
    "    kf = pos_vel_filter(x0, R=R, P=P, Q=Q, dt=dt)  \n",
    "\n",
    "    # run the kalman filter and store the results\n",
    "    xs, cov = [], []\n",
    "    for z in zs:\n",
    "        kf.predict()\n",
    "        kf.update(z)\n",
    "        xs.append(kf.x)\n",
    "        cov.append(kf.P)\n",
    "\n",
    "    xs, cov = np.array(xs), np.array(cov)\n",
    "    if do_plot:\n",
    "        plot_track(xs[:, 0], track, zs, cov, **kwargs)\n",
    "    return xs, cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the complete code for the filter, and most of it is boilerplate. I've made it flexible enough to support several uses in this chapter, so it is a bit verbose. Let's work through it line by line. \n",
    "\n",
    "The first lines checks to see if you provided it with measurement data in `data`. If not, it creates the data using the `compute_dog_data` function we wrote earlier.\n",
    "\n",
    "The next lines uses our helper function to create a Kalman filter.\n",
    "\n",
    "```python\n",
    "# create the Kalman filter\n",
    " kf = pos_vel_filter(x0, R=R, P=P, Q=Q, dt=dt)\n",
    "```\n",
    "\n",
    "All we need to do is perform the update and predict steps of the Kalman filter for each measurement. The `KalmanFilter` class provides the two methods `update()` and `predict()` for this purpose. `update()` performs the measurement update step of the Kalman filter, and so it takes a variable containing the sensor measurement. \n",
    "\n",
    "Absent the work of storing the results, the loop reads:\n",
    "\n",
    "```python\n",
    "    for z in zs:\n",
    "        kf.predict()\n",
    "        kf.update(z)\n",
    "```\n",
    "\n",
    "Each call to `predict` and `update` modifies the state variables `x` and `P`. Therefore, after the call to `predict`, `kf.x` contains the prior. After the call to update, `kf.x` contains the posterior. `data` contains the actual position and measurement of the dog, so we use `[:, 1]` to get an array of measurements.\n",
    "\n",
    "It really cannot get much simpler than that. As we tackle more complicated problems this code will remain largely the same; all of the work goes into setting up the `KalmanFilter` matrices; executing the filter is trivial.\n",
    "\n",
    "The rest of the code optionally plots the results and then returns the saved states and covariances.\n",
    "\n",
    "Let's run it. We have 50 measurements with a noise variance of 10 and a process variance of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.diag([500., 49.])\n",
    "Ms, Ps = run(count=50, R=10, Q=0.01, P=P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot to learn, but we have implemented a Kalman filter using the same theory and equations as published by Rudolf Kalman! Code very much like this runs inside of your GPS, airliners, robots, and so on. \n",
    "\n",
    "The first plot plots the output of the Kalman filter against the measurements and the actual position of our dog (labelled *Track*). After the initial settling in period the filter should track the dog's position very closely. The yellow shaded portion between the black dotted lines shows 1 standard deviations of the filter's variance, which I explain in the next paragraph.\n",
    "\n",
    "The next two plots show the variance of $x$ and of $\\dot x$. I have plotted the diagonals of $\\mathbf P$ over time. Recall that the diagonal of a covariance matrix contains the variance of each state variable. So $\\mathbf P[0,0]$ is the variance of $x$, and $\\mathbf P[1,1]$ is the variance of $\\dot x$. You can see that we quickly converge to small variances for both. \n",
    "\n",
    "The covariance matrix $\\mathbf P$ tells us the *theoretical* performance of the filter *assuming* everything we tell it is true. Recall that the standard deviation is the square root of the variance, and that approximately 68% of a Gaussian distribution occurs within one standard deviation. If at least 68% of the filter output is within one standard deviation  the filter may be performing well. In the top chart I have displayed the one standard deviation as the yellow shaded area between the two dotted lines. To my eye it looks like perhaps the filter is slightly exceeding that bounds, so the filter probably needs some tuning.\n",
    "\n",
    "In the univariate chapter we filtered very noisy signals with much simpler code than the code above. However, realize that right now we are working with a very simple example - an object moving through 1-D space and one sensor. That is about the limit of what we can compute with the code in the last chapter. In contrast, we can implement very complicated, multidimensional filters with this code merely by altering our assignments to the filter's variables. Perhaps we want to track 100 dimensions in financial models. Or we have an aircraft with a GPS, INS, TACAN, radar altimeter, baro altimeter, and airspeed indicator, and we want to integrate all those sensors into a model that predicts position, velocity, and acceleration in 3D space. We can do that with the code in this chapter.\n",
    "\n",
    "I want you to get a better feel for how the Gaussians change over time, so here is a 3D plot showing the Gaussians every 7th epoch (time step). Every 7th separates them enough so can see each one independently. The first Gaussian at $t=0$ is to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kf_book.book_plots import set_figsize, figsize\n",
    "from kf_book.nonlinear_plots import plot_gaussians\n",
    "\n",
    "P = np.diag([3., 1.])\n",
    "np.random.seed(3)\n",
    "Ms, Ps = run(count=25, R=10, Q=0.01, P=P, do_plot=False)\n",
    "with figsize(x=9, y=5):\n",
    "    plot_gaussians(Ms[::7], Ps[::7], (-5,25), (-5, 5), 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Saver Class\n",
    "\n",
    "In the `run()` method I wrote boilerplate code to save the results of the filter\n",
    "```python\n",
    "    xs, cov = [], []\n",
    "    for z in zs:\n",
    "        kf.predict()\n",
    "        kf.update(z)\n",
    "        xs.append(kf.x)\n",
    "        cov.append(kf.P)\n",
    "\n",
    "    xs, cov = np.array(xs), np.array(cov)\n",
    "```\n",
    "\n",
    "There's an easy way to avoid this. `filtery.common` provides the `Saver` class which will save all attributes in the Kalman filter class each time `Saver.save()` is called. Let's see it in action and then we will talk about it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.common import Saver\n",
    "kf = pos_vel_filter([0, .1], R=R, P=P, Q=Q, dt=1.) \n",
    "s = Saver(kf)\n",
    "for i in range(1, 6):\n",
    "    kf.predict()\n",
    "    kf.update([i])\n",
    "    s.save()  # save the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Saver` object now contains lists of all the attributes of the KalmanFilter object. `kf.x` is the current state estimate of the filter. Therefore `s.x` contains the saved state estimate that was computed inside the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all the available attributes with the `keys` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many attributes there that we haven't discussed yet, but many should be familar.\n",
    "\n",
    "At this point you could write code to plot any of these variables. However, it is often more useful to use `np.array` instead of lists. Calling `Saver.to_array()` will convert the lists into `np.array`. There is one caveat: if the shape of any of the attributes changes during the run, the `to_array` will raise an exception since `np.array` requires all of the elements to be of the same type and size. \n",
    "\n",
    "If you look at the keys again you'll see that `z` is one of the choices. This is promising; apparently the measurement `z` is saved for us. Let's plot it against the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s.to_array()\n",
    "book_plots.plot_measurements(s.z);\n",
    "plt.plot(s.x[:, 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I've demonstrated this with the `KalmanFilter` class, it will work with all filter classes implemented by `FilterPy`. It will probably work with any class you write as well, as it inspects the object to retrieve the attribute names. We will use this class throughout the book to keep the code readable and short. Using the `Saver` will slow down your code because a lot happens behind the scenes, but for learning and exploring the convience cannot be beat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kalman Filter Equations\n",
    "\n",
    "We are now ready to learn how `predict()` and `update()` perform their computations. \n",
    "\n",
    "A word about notation. I'm a programmer, and I am used to code that reads\n",
    "\n",
    "```python\n",
    "x = x + 1\n",
    "``` \n",
    "\n",
    "That is not an equation as the sides are not equal, but an *assignment*. If we wanted to write this in mathematical notation we'd write\n",
    "$$x_k = x_{k-1} + 1$$\n",
    "\n",
    "Kalman filter equations are littered with subscripts and superscripts to keep the equations mathematically consistent. I find this makes them extremely hard to read. In most of the book I opt for subscriptless assignments. As a programmer you should understand that I am showing you assignments which implement an algorithm that is to be executed step by step. I'll elaborate on this once we have a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Equations\n",
    "\n",
    "The Kalman filter uses these equations to compute the *prior* - the predicted next state of the system. They compute the prior mean ($\\bar{\\mathbf x}$)  and covariance ($\\bar{\\mathbf P}$) of the system.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{\\bar x} &= \\mathbf{Fx} + \\mathbf{Bu}\\\\\n",
    "\\mathbf{\\bar P} &= \\mathbf{FPF}^\\mathsf T + \\mathbf Q\n",
    "\\end{aligned}$$\n",
    "\n",
    "$\\underline{\\textbf{Mean}}$\n",
    "\n",
    "$\\mathbf{\\bar x} = \\mathbf{Fx} + \\mathbf{Bu}$\n",
    "\n",
    "As a reminder, the linear equation $\\mathbf{Ax} = \\mathbf b$ represents a system of equations, where $\\mathbf A$ holds the coefficients set of equations, $\\mathbf x$ is the vector of variables. Performing the multiplication $\\mathbf{Ax}$ computes the right hand side values for that set of equations, represented by $\\mathbf b$.\n",
    "\n",
    "If $\\mathbf F$ contains the state transition for a given time step, then the product $\\mathbf{Fx}$ computes the state after that transition. Easy! Likewise, $\\mathbf B$ is the control function, $\\mathbf u$ is the control input, so $\\mathbf{Bu}$ computes the contribution of the controls to the state after the transition. Thus, the prior $\\mathbf{\\bar x}$ is computed as the sum of $\\mathbf{Fx}$ and $\\mathbf{Bu}$.\n",
    "\n",
    "The equivalent univariate equation is\n",
    "\n",
    "$$\\bar\\mu = \\mu + \\mu_{move}$$\n",
    "\n",
    "If you perform the matrix multiplication $\\mathbf{Fx}$ it generates this equation for $x$.\n",
    "\n",
    "Let's make this explicit. Recall the value for $\\mathbf F$ from the last chapter:\n",
    "\n",
    "$$\\mathbf F = \\begin{bmatrix}1&\\Delta t  \\\\ 0&1\\end{bmatrix}$$\n",
    "\n",
    "Thus $\\mathbf{\\bar x} = \\mathbf{Fx}$ corresponds to the set of linear equations:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\begin{aligned}\n",
    "\\bar x &= 1x + &\\Delta t\\, \\dot x \\\\\n",
    "\\bar{\\dot x} &=0x + &1\\, \\dot x\n",
    "\\end{aligned}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Covariance}}$\n",
    "\n",
    "$\\mathbf{\\bar P} = \\mathbf{FPF}^\\mathsf T + \\mathbf Q$\n",
    "\n",
    "This equation is not as easy to understand so we will spend more time on it. \n",
    "\n",
    "In univariate version of this equation is:\n",
    "\n",
    "$$\\bar\\sigma^2 = \\sigma^2 + \\sigma^2_{move}$$\n",
    "\n",
    "We add the variance of the movement to the variance of our estimate to reflect the loss of knowlege. We need to do the same thing here, except it isn't quite that easy with multivariate Gaussians. \n",
    "\n",
    "We can't simply write $\\mathbf{\\bar P} = \\mathbf P + \\mathbf Q$. In a multivariate Gaussians the state variables are *correlated*. What does this imply? Our knowledge of the velocity is imperfect, but we are adding it to the position with\n",
    "\n",
    "$$\\bar x = \\dot x\\Delta t + x$$\n",
    "\n",
    "Since we do not have perfect knowledge of the value of $\\dot x$ the sum $\\bar x = \\dot x\\Delta t + x$ gains uncertainty. Because the positions and velocities are correlated we cannot simply add the covariance matrices. For example, if $\\mathbf P$ and $\\mathbf Q$ are diagonal matrices the sum would also be diagonal. But we know position is correlated to velocity so the off-diagonal elements should be non-zero. \n",
    "\n",
    "The correct equation is\n",
    "\n",
    "$$\\mathbf{\\bar P} = \\mathbf{FPF}^\\mathsf T + \\mathbf Q$$\n",
    "\n",
    "Expressions in the form $\\mathbf{ABA}^\\mathsf T$ are common in linear algebra. You can think of it as *projecting* the middle term by the outer term. We will be using this many times in the rest of the book. I admit this may be a 'magical' equation to you. Let's explore it.\n",
    "\n",
    "When we initialize $\\mathbf P$ with\n",
    "\n",
    "$$\\mathbf P = \\begin{bmatrix}\\sigma^2_x & 0 \\\\ 0 & \\sigma^2_v\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "the value for $\\mathbf{FPF}^\\mathsf T$ is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{FPF}^\\mathsf T &= \\begin{bmatrix}1&\\Delta t\\\\0&1\\end{bmatrix}\n",
    "\\begin{bmatrix}\\sigma^2_x & 0 \\\\  0 & \\sigma^2_{v}\\end{bmatrix}\n",
    "\\begin{bmatrix}1&0\\\\\\Delta t&1\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\\sigma^2_x&\\sigma_v^2\\Delta t\\\\  0 & \\sigma^2_{v}\\end{bmatrix}\n",
    "\\begin{bmatrix}1&0\\\\\\Delta t&1\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\\sigma^2_x +  \\sigma_v^2\\Delta t^2  &  \\sigma_v^2\\Delta t \\\\\n",
    "\\sigma_v^2\\Delta t & \\sigma^2_{v}\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The initial value for $\\mathbf P$ had no covariance between the position and velocity.  Position is computed as $\\dot x\\Delta t + x$, so there is a correlation between the position and velocity. The multiplication $\\mathbf{FPF}^\\mathsf T$ computes a covariance of $\\sigma_v^2 \\Delta t$. The exact value is not important; you just need to recognize that $\\mathbf{FPF}^\\mathsf T$ uses the process model to automatically compute the covariance between the position and velocity!\n",
    "\n",
    "Another way to think of this is to reflect on the $\\mathbf{Fx}$ multiplication. That projected $\\mathbf x$ forward in time. $\\mathbf {FP}$ might seem to be the equivalent operation, but $\\mathbf P$ is a matrix while $\\mathbf x$ is a vector. The trailing $\\mathbf F^\\mathsf T$ term ensures that we multiply by both the rows and columns of $\\mathbf F$. In the second line of the computation of $\\mathbf{FPF}^\\mathsf T$ we have the value for $\\mathbf{FP}$. You can see that it is an upper triangular matrix because we haven't fully incorporated $\\mathbf F$ into the multiplication.\n",
    "\n",
    "If you have some experience with linear algebra and statistics, this may help. The covariance due to the prediction can be modeled as the expected value of the error in the prediction step, given by this equation. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\bar{\\mathbf P} &= \\mathbb E[(\\mathbf{Fx - \\bar \\mu})(\\mathbf{Fx - \\bar\\mu})^\\mathsf T]\\\\\n",
    " &= \\mathbf F\\, \\mathbb E[\\mathbf{(x- \\bar\\mu)(x- \\bar\\mu)}^\\mathsf T]\\, \\mathbf F^\\mathsf T\n",
    "\\end{aligned}$$\n",
    "\n",
    "Of course, $\\mathbb E[\\mathbf{(x- \\bar\\mu)(x- \\bar\\mu)}^\\mathsf T]$ is just $\\mathbf P$, giving us\n",
    "\n",
    "$$\\bar{\\mathbf P} = \\mathbf{FPF}^\\mathsf T$$\n",
    "\n",
    "Let's look at its effect. Here I use $\\mathbf F$ from our filter and project the state forward 6/10ths of a second. I do this five times so you can see how $\\mathbf{\\bar P}$ continues to change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.6\n",
    "x = np.array([0., 5.])\n",
    "F = np.array([[1., dt], [0, 1.]])\n",
    "P = np.array([[1.5, 0], [0, 3.]])\n",
    "plot_covariance_ellipse(x, P, edgecolor='r')\n",
    "\n",
    "for _ in range(5):\n",
    "    x = F @ x\n",
    "    P = F @ P @ F.T\n",
    "    plot_covariance_ellipse(x, P, edgecolor='k', ls='dashed')\n",
    "book_plots.set_labels(x='position', y='velocity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that with a velocity of 5 the position correctly moves 3 units in each 6/10ths of a second step. At each step the width of the ellipse is larger, indicating that we have lost information about the position due to adding $\\dot x\\Delta t$ to x at each step. The height has not changed - our system model says the velocity does not change, so the belief we have about the velocity cannot change. As time continues you can see that the ellipse becomes more and more tilted. Recall that a tilt indicates *correlation*. $\\mathbf F$ linearly correlates $x$ with $\\dot x$ with the expression $\\bar x = \\dot x \\Delta t + x$. The $\\mathbf{FPF}^\\mathsf T$ computation correctly incorporates this correlation into the covariance matrix.\n",
    "\n",
    "Here is an animation of this equation that allows you to change the design of $\\mathbf F$ to see how it affects shape of $\\mathbf P$. The `F00` slider affects the value of F[0, 0]. `covar` sets the intial covariance between the position and velocity($\\sigma_x\\sigma_{\\dot x}$). I recommend answering these questions at a minimum\n",
    "\n",
    "* what if $x$ is not correlated to $\\dot x$? (set F01 to 0, the rest at defaults)\n",
    "* what if $x = 2\\dot x\\Delta t + x_0$? (set F01 to 2, the rest at defaults)\n",
    "* what if $x = \\dot x\\Delta t + 2x_0$? (set F00 to 2, the rest at defaults)\n",
    "* what if $x = \\dot x\\Delta t$?  (set F00 to 0, the rest at defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from kf_book.book_plots import IntSlider, FloatSlider\n",
    "\n",
    "def plot_FPFT(F00, F01, F10, F11, covar):   \n",
    "    plt.figure()\n",
    "    dt = 1.\n",
    "    x = np.array((0, 0.))\n",
    "    P = np.array(((1, covar), (covar, 2)))\n",
    "    F = np.array(((F00, F01), (F10, F11)))\n",
    "    plot_covariance_ellipse(x, P)\n",
    "    plot_covariance_ellipse(x, F @ P @ F.T, ec='r')\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlim(-4, 4)\n",
    "    plt.ylim(-4, 4)\n",
    "    #plt.title(str(F))\n",
    "    plt.xlabel('position')\n",
    "    plt.ylabel('velocity')\n",
    "                 \n",
    "interact(plot_FPFT, \n",
    "         F00=IntSlider(value=1, min=0, max=2), \n",
    "         F01=FloatSlider(value=1, min=0, max=2, description='F01(dt)'),\n",
    "         F10=FloatSlider(value=0, min=0, max=2),\n",
    "         F11=FloatSlider(value=1, min=0, max=2),\n",
    "         covar=FloatSlider(value=0, min=0, max=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you are reading this in a static form: instructions to run this online are here: https://git.io/vza7b). Or, go to binder using the link below, and open this notebook from there.\n",
    "\n",
    "http://mybinder.org/repo/rlabbe/Kalman-and-Bayesian-Filters-in-Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Equations\n",
    "\n",
    "The update equations look messier than the predict equations, but that is mostly due to the Kalman filter computing the update in measurement space. This is because measurements are not *invertible*. For example, consider a  sensor that gives the range to a target. It is impossible to convert a range into a position - an infinite number of positions in a circle will yield the same range. On the other hand, we can always compute the range (measurement) given a position (state).\n",
    "\n",
    "Before I continue, recall that we are trying to do something very simple: choose a new estimate chosen somewhere between a measurement and a prediction, as in this chart:\n",
    "<img src=\"./figs/residual_chart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations will be complicated because the state has multiple dimensions, but this operations is what we are doing. Don't let the equations distract you from the simplicity of this idea.\n",
    "\n",
    "$\\underline{\\textbf{System Uncertainty}}$\n",
    "\n",
    "$\\textbf{S} = \\mathbf{H\\bar PH}^\\mathsf T + \\mathbf R$\n",
    "\n",
    "To work in measurement space the Kalman filter has to project the covariance matrix into measurement space. The math for this is $\\mathbf{H\\bar PH}^\\mathsf T$, where $\\mathbf{\\bar P}$ is the *prior* covariance and $\\mathbf H$ is the measurement function.\n",
    "\n",
    "\n",
    "You should recognize this $\\mathbf{ABA}^\\mathsf T$ form - the prediction step used $\\mathbf{FPF}^\\mathsf T$ to update $\\mathbf P$ with the state transition function. Here, we use the same form to update it with the measurement function. The linear algebra is changing the coordinate system for us. \n",
    "\n",
    "Once the covariance is in measurement space we need to account for the sensor noise. This is very easy  - we just add matrices. The result is variously called the *system uncertainty* or *innovation covariance*.\n",
    "\n",
    "If you ignore the $\\mathbf H$ term this equation is the equivalent to the denominator in the univariate equation for the Kalman gain:\n",
    "\n",
    "$$K = \\frac {\\bar\\sigma^2} {\\bar\\sigma^2 + \\sigma_z^2}$$\n",
    "\n",
    "Compare the equations for the system uncertainty and the covariance\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{S} &= \\mathbf{H\\bar PH}^\\mathsf T + \\mathbf R\\\\\n",
    "\\mathbf{\\bar P} &= \\mathbf{FPF}^\\mathsf T + \\mathbf Q\n",
    "\\end{aligned}$$\n",
    "\n",
    "In each equation $\\mathbf P$ is put into a different space with either the function $\\mathbf H$ or $\\mathbf F$. Then we add the noise matrix associated with that space.\n",
    "\n",
    "$\\underline{\\textbf{Kalman Gain}}$\n",
    "\n",
    "$\\mathbf K = \\mathbf{\\bar PH}^\\mathsf T \\mathbf{S}^{-1}$\n",
    "\n",
    "Look back at the residual diagram. Once we have a prediction and a measurement we need to select an estimate somewhere between the two. If we have more certainty about the measurement the estimate will be closer to it. If instead we have more certainty about the prediction then the estimate will be closer to it. \n",
    "\n",
    "In the univariate chapter we scaled the mean using this equation\n",
    "\n",
    "$$\n",
    "\\mu =\\frac{\\bar\\sigma^2 \\mu_z + \\sigma_\\mathtt{z}^2 \\bar\\mu} {\\bar\\sigma^2 + \\sigma_\\mathtt{z}^2}$$\n",
    "\n",
    "which we simplified to\n",
    "\n",
    "$$\\mu = (1-K)\\bar\\mu + K\\mu_\\mathtt{z}$$\n",
    "\n",
    "which gave us\n",
    "\n",
    "$$K = \\frac {\\bar\\sigma^2} {\\bar\\sigma^2 + \\sigma_z^2}$$\n",
    "\n",
    "$K$ is the *Kalman gain*, and it is a real number between 0 and 1. Ensure you understand how it selects a mean somewhere between the prediction and measurement. The Kalman gain is a *percentage* or *ratio* - if K is .9 it takes 90% of the measurement and 10% of the prediction. \n",
    "\n",
    "For the multivariate Kalman filter $\\mathbf K$ is a vector, not a scalar. Here is the equation again: $\\mathbf K = \\mathbf{\\bar PH}^\\mathsf T \\mathbf{S}^{-1}$. Is this a *ratio*? We can think of the inverse of a matrix as linear algebra's way of finding the reciprocal. Division is not defined for matrices, but it is useful to think of it in this way. So we can read the equation for $\\textbf{K}$ as meaning\n",
    "\n",
    "$$\\begin{aligned} \\mathbf K &\\approx \\frac{\\mathbf{\\bar P}\\mathbf H^\\mathsf T}{\\mathbf{S}} \\\\\n",
    "\\mathbf K &\\approx \\frac{\\mathsf{uncertainty}_\\mathsf{prediction}}{\\mathsf{uncertainty}_\\mathsf{prediction} + \\mathsf{uncertainty}_\\mathsf{measurement}}\\mathbf H^\\mathsf T\n",
    "\\end{aligned}$$\n",
    "\n",
    "The Kalman gain equation computes a ratio based on how much we trust the prediction vs the measurement. We did the same thing in every prior chapter. The equation is complicated because we are doing this in multiple dimensions via matrices, but the concept is simple. The $\\mathbf H^\\mathsf T$ term is less clear, I'll explain it soon. If you ignore that term the equation for the Kalman gain is the same as the univariate case: divide the uncertainty of the prior with the of the sum of the uncertainty of the prior and measurement.\n",
    "\n",
    "$\\underline{\\textbf{Residual}}$\n",
    "\n",
    "$\\mathbf y = \\mathbf z - \\mathbf{H\\bar{x}}$\n",
    "\n",
    "This is an easy one as we've covered this equation while designing the measurement function $\\mathbf H$. Recall that the measurement function converts a state into a measurement. So $\\mathbf{Hx}$ converts $\\mathbf x$ into an equivalent measurement. Once that is done, we can subtract it from the measurement $\\mathbf z$ to get the residual - the difference between the measurement and prediction.\n",
    "\n",
    "The univariate equation is\n",
    "\n",
    "$$y = z - \\bar x$$\n",
    "\n",
    "and clearly computes the same thing, but only in one dimension.\n",
    "\n",
    "$\\underline{\\textbf{State Update}}$\n",
    "\n",
    "$\\mathbf x = \\mathbf{\\bar x} + \\mathbf{Ky}$\n",
    "\n",
    "We select our new state to be along the residual, scaled by the Kalman gain. The scaling is performed by $\\mathbf{Ky}$, which both scales the residual and converts it back into state space with the $\\mathbf H^\\mathsf T$ term which is in $\\mathbf K$. This is added to the prior, yielding the equation: $\\mathbf x =\\mathbf{\\bar x} + \\mathbf{Ky}$. Let me write out $\\mathbf K$ so we can see the entire computation:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf x &= \\mathbf{\\bar x} + \\mathbf{Ky} \\\\\n",
    "&= \\mathbf{\\bar x} + \\mathbf{\\bar PH}^\\mathsf T \\mathbf{S}^{-1}\\mathbf y \\\\\n",
    "&\\approx \\mathbf{\\bar x} + \\frac{\\mathsf{uncertainty}_\\mathsf{prediction}}{\\mathsf{uncertainty}_\\mathsf{measurement}}\\mathbf H^\\mathsf T\\mathbf y\n",
    "\\end{aligned}$$\n",
    "\n",
    "Perhaps a better way to *see* the ratio is to rewrite the estimate equation:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf x &= \\mathbf{\\bar x} + \\mathbf{Ky} \\\\\n",
    "&= \\mathbf{\\bar x} +\\mathbf K(\\mathbf z - \\mathbf{H\\bar x}) \\\\\n",
    "&= (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar x} + \\mathbf{Kz}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The similarity between this and the univariate form should be obvious:\n",
    "$$\\mu = (1-K)\\bar\\mu + K\\mu_\\mathtt{z}$$\n",
    "\n",
    "$\\underline{\\textbf{Covariance Update}}$\n",
    "\n",
    "$\\mathbf P = (\\mathbf{I}-\\mathbf{KH})\\mathbf{\\bar P}$\n",
    "\n",
    "$\\mathbf{I}$ is the identity matrix, and is the way we represent $1$ in multiple dimensions. $\\mathbf H$ is our measurement function, and is a constant.  We can think of the equation as $\\mathbf P = (1-c\\mathbf K)\\mathbf P$. $\\mathbf K$ is our ratio of how much prediction vs measurement we use. If $\\mathbf K$ is large then $(1-\\mathbf{cK})$ is small, and $\\mathbf P$ will be made smaller than it was. If $\\mathbf K$ is small, then $(1-\\mathbf{cK})$ is large, and $\\mathbf P$ will be relatively larger. This means that we adjust the size of our uncertainty by some factor of the Kalman gain.\n",
    "\n",
    "This equation can be numerically unstable and I don't use it in FilterPy. The subtraction can destroy symmetry and lead to floating point errors over time. Later I'll share more complicated but numerically stable forms of this equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example not using FilterPy\n",
    "\n",
    "FilterPy hides the details of the implementation from us. Normally you will appreciate this, but let's implement the last filter without FilterPy. To do so we need to define our matrices as variables, and then implement the Kalman filter equations explicitly.\n",
    "\n",
    "Here we initialize our matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1.\n",
    "R_var = 10\n",
    "Q_var = 0.01\n",
    "x = np.array([[10.0, 4.5]]).T\n",
    "P = np.diag([500, 49])\n",
    "F = np.array([[1, dt],\n",
    "              [0,  1]])\n",
    "H = np.array([[1., 0.]])\n",
    "R = np.array([[R_var]])\n",
    "Q = Q_discrete_white_noise(dim=2, dt=dt, var=Q_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import inv\n",
    "\n",
    "count = 50\n",
    "track, zs = compute_dog_data(R_var, Q_var, count)\n",
    "xs, cov = [], []\n",
    "for z in zs:\n",
    "    # predict\n",
    "    x = F @ x\n",
    "    P = F @ P @ F.T + Q\n",
    "    \n",
    "    #update\n",
    "    S = H @ P @ H.T + R\n",
    "    K = P @ H.T @ inv(S)\n",
    "    y = z - H @ x\n",
    "    x += K @ y\n",
    "    P = P - K @ H @ P\n",
    "    \n",
    "    xs.append(x)\n",
    "    cov.append(P)\n",
    "\n",
    "xs, cov = np.array(xs), np.array(cov)\n",
    "plot_track(xs[:, 0], track, zs, cov, plot_P=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are identical to the FilterPy version. Which you prefer is up to you. I prefer not polluting my namespace with variables such as `x`, `P`, and so on; `dog_filter.x` is, to me, more readable.\n",
    "\n",
    "More importantly, this example requires you to remember and program the equations for the Kalman filter. Sooner or later you will make a mistake. FilterPy's version ensures that your code will be correct. On the other hand, if you make a mistake in your definitions, such as making $\\mathbf H$ a column vector instead of a row vector, FilterPy's error message will be harder to debug than this explicit code. \n",
    "\n",
    "FilterPy's KalmanFilter class provides additional functionality such as smoothing, batch processing, faded memory filtering, computation of the maximum likelihood function, and more. You get all of this functionality without having to explicitly program it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have learned the Kalman filter equations. Here they are all together for your review. There was a lot to learn, but I hope that as you went through each you recognized it's kinship with the equations in the univariate filter. In the *Kalman Math* chapter I will show you that if we set the dimension of $\\mathbf x$ to one that these equations revert back to the equations for the univariate filter. This is not \"like\" the univariate filter - it is a multidimensional implementation of it.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Predict Step}\\\\\n",
    "\\mathbf{\\bar x} &= \\mathbf{F x} + \\mathbf{B u} \\\\\n",
    "\\mathbf{\\bar P} &= \\mathbf{FP{F}}^\\mathsf T + \\mathbf Q \\\\\n",
    "\\\\\n",
    "\\text{Update Step}\\\\\n",
    "\\textbf{S} &= \\mathbf{H\\bar PH}^\\mathsf T + \\mathbf R \\\\\n",
    "\\mathbf K &= \\mathbf{\\bar PH}^\\mathsf T \\mathbf{S}^{-1} \\\\\n",
    "\\textbf{y} &= \\mathbf z - \\mathbf{H \\bar x} \\\\\n",
    "\\mathbf x &=\\mathbf{\\bar x} +\\mathbf{K\\textbf{y}} \\\\\n",
    "\\mathbf P &= (\\mathbf{I}-\\mathbf{KH})\\mathbf{\\bar P}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "I want to share a form of the equations that you will see in the literature. There are many different notation systems used, but this gives you an idea of what to expect.\n",
    "\n",
    " $$\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf x}_{k\\mid k-1} &= \\mathbf F_k\\hat{\\mathbf x}_{k-1\\mid k-1} + \\mathbf B_k \\mathbf u_k  \\\\\n",
    "\\mathbf P_{k\\mid k-1} &=  \\mathbf F_k \\mathbf P_{k-1\\mid k-1} \\mathbf F_k^\\mathsf T + \\mathbf Q_k \\\\        \t\n",
    "\\tilde{\\mathbf y}_k &= \\mathbf z_k - \\mathbf H_k\\hat{\\mathbf x}_{k\\mid k-1}\\\\\n",
    "\\mathbf{S}_k &= \\mathbf H_k \\mathbf P_{k\\mid k-1} \\mathbf H_k^\\mathsf T + \\mathbf R_k \\\\\n",
    "\\mathbf K_k &= \\mathbf P_{k\\mid k-1}\\mathbf H_k^\\mathsf T \\mathbf{S}_k^{-1}\\\\\n",
    "\\hat{\\mathbf x}_{k\\mid k} &= \\hat{\\mathbf x}_{k\\mid k-1} + \\mathbf K_k\\tilde{\\mathbf y}_k\\\\\n",
    "\\mathbf P_{k|k} &= (I - \\mathbf K_k \\mathbf H_k) \\mathbf P_{k|k-1}\n",
    "\\\\\\end{aligned}\n",
    "$$\n",
    "\n",
    "This notation uses the Bayesian $a\\mid b$ notation, which means $a$ given the evidence of $b$. The hat means estimate. Thus $\\hat{\\mathbf x}_{k\\mid k}$ means the estimate of the state $\\mathbf x$ at step $k$ (the first k) given the evidence from step $k$ (the second k). The posterior, in other words. $\\hat{\\mathbf x}_{k\\mid k-1}$ means the estimate for the state $\\mathbf x$ at step $k$ given the estimate from step $k - 1$. The prior, in other words. \n",
    "\n",
    "This notation, copied from [Wikipedia](https://en.wikipedia.org/wiki/Kalman_filter#Details) [[1]](#[wiki_article]), allows a mathematician to express himself exactly. In formal publications presenting new results this precision is necessary. As a programmer I find it fairly unreadable. I am used to thinking about variables changing state as a program runs, and do not use a different variable name for each new computation. There is no agreed upon format in the literature, so each author makes different choices. I find it challenging to switch quickly between books and papers, and so have adopted my admittedly less precise notation. Mathematicians may write scathing emails to me, but I hope programmers and students will rejoice at my simplified notation.\n",
    "\n",
    "The **Symbology** Appendix lists the notation used by various authors. This brings up another difficulty. Different authors use different variable names. $\\mathbf x$ is fairly universal, but after that it is anybody's guess. For example, it is common to use $\\mathbf{A}$ for what I call $\\mathbf F$. You must read carefully, and hope that the author defines their variables (they often do not).\n",
    "\n",
    "If you are a programmer trying to understand a paper's equations, I suggest starting by removing all of the superscripts, subscripts, and diacriticals, replacing them with a single letter. If you work with equations like this every day this is superfluous advice, but when I read I am usually trying to understand the flow of computation. To me it is far more understandable to remember that $P$ in this step represents the updated value of $P$ computed in the last step, as opposed to trying to remember what $P_{k-1}(+)$ denotes, and what its relation to $P_k(-)$ is, if any, and how any of that relates to the completely different notation used in the paper I read 5 minutes ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Show Effect of Hidden Variables\n",
    "\n",
    "In our filter velocity is a hidden variable. How would a filter perform if we did not use velocity in the state?\n",
    "\n",
    "Write a Kalman filter that uses the state $\\mathbf x=\\begin{bmatrix}x\\end{bmatrix}$ and compare it against a filter that uses $\\mathbf x=\\begin{bmatrix}x & \\dot x\\end{bmatrix}^\\mathsf T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We've already implemented a Kalman filter for position and velocity, so I will provide the code without much comment, and then plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy.random import randn\n",
    "\n",
    "def univariate_filter(x0, P, R, Q):\n",
    "    f = KalmanFilter(dim_x=1, dim_z=1, dim_u=1)\n",
    "    f.x = np.array([[x0]])\n",
    "    f.P *= P\n",
    "    f.H = np.array([[1.]])\n",
    "    f.F = np.array([[1.]])\n",
    "    f.B = np.array([[1.]])\n",
    "    f.Q *= Q\n",
    "    f.R *= R\n",
    "    return f\n",
    "\n",
    "def plot_1d_2d(xs, xs1d, xs2d):\n",
    "    plt.plot(xs1d, label='1D Filter')\n",
    "    plt.scatter(range(len(xs2d)), xs2d, c='r', alpha=0.7, label='2D Filter')\n",
    "    plt.plot(xs, ls='--', color='k', lw=1, label='track')\n",
    "    plt.title('State')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    \n",
    "def compare_1D_2D(x0, P, R, Q, vel, u=None):\n",
    "    # storage for filter output\n",
    "    xs, xs1, xs2 = [], [], []\n",
    "\n",
    "    # 1d KalmanFilter\n",
    "    f1D = univariate_filter(x0, P, R, Q)\n",
    "\n",
    "    #2D Kalman filter\n",
    "    f2D = pos_vel_filter(x=(x0, vel), P=P, R=R, Q=0)\n",
    "    if np.isscalar(u):\n",
    "        u = [u]\n",
    "    pos = 0 # true position\n",
    "    for i in range(100):\n",
    "        pos += vel\n",
    "        xs.append(pos)\n",
    "\n",
    "        # control input u - discussed below\n",
    "        f1D.predict(u=u)\n",
    "        f2D.predict()\n",
    "        \n",
    "        z = pos + randn()*sqrt(R) # measurement\n",
    "        f1D.update(z)\n",
    "        f2D.update(z)\n",
    "        \n",
    "        xs1.append(f1D.x[0])\n",
    "        xs2.append(f2D.x[0])\n",
    "    plt.figure()\n",
    "    plot_1d_2d(xs, xs1, xs2)\n",
    "\n",
    "compare_1D_2D(x0=0, P=50., R=5., Q=.02, vel=1.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The filter that incorporates velocity into the state produces much better estimates than the filter that only tracks position. The univariate filter has no way to estimate the velocity or change in position, so it lags the tracked object. \n",
    "\n",
    "In the univarate Kalman filter chapter we had a control input `u` to the predict equation:\n",
    "\n",
    "```python\n",
    "    def predict(self, u=0.0):\n",
    "        self.x += u\n",
    "        self.P += self.Q\n",
    "```\n",
    "\n",
    "Let's try specifying the control input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_1D_2D(x0=0, P=50., R=5., Q=.02, vel=1., u=1.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the performance of the two filters are similar, and perhaps the univariate filter is tracking more cloesly. But let's see what happens when the actual velocity `vel` is different from the control input `u`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_1D_2D(x0=0, P=50., R=5., Q=.02, vel=-2., u=1.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are tracking a robot which we are also controlling the univariate filter can do a very good job because the control input allows the filter to make an accurate prediction. But if we are tracking passively the control input is not much help unless we can make an accurate *apriori* guess as to the velocity. This is rarely possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Velocity is Calculated\n",
    "\n",
    "I haven't explained how the filter computes the velocity, or any hidden variable. If we plug in the values we calculated for each of the filter's matrices we can see what happens.\n",
    "\n",
    "First we need to compute the system uncertainty.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\textbf{S} &= \\mathbf{H\\bar PH}^\\mathsf T + \\mathbf R \\\\\n",
    "&= \\begin{bmatrix} 1 & 0\\end{bmatrix}\n",
    "\\begin{bmatrix}\\sigma^2_x & \\sigma_{xv} \\\\ \\sigma_{xv} & \\sigma^2_v\\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}\\sigma_z^2\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\\sigma_x^2 & \\sigma_{xv}\\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}+ \\begin{bmatrix}\\sigma_z^2\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\\sigma_x^2 +\\sigma_z^2\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now that we have $\\mathbf S$ we can find the value for the Kalman gain:\n",
    "$$\\begin{aligned}\n",
    "\\mathbf K &= \\mathbf{\\bar PH}^\\mathsf T \\mathbf{S}^{-1} \\\\\n",
    "&= \\begin{bmatrix}\\sigma^2_x & \\sigma_{xv} \\\\ \\sigma_{xv} & \\sigma^2_v\\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}\n",
    "\\begin{bmatrix}\\frac{1}{\\sigma_x^2 +\\sigma_z^2}\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\\sigma^2_x  \\\\ \\sigma_{xv}\\end{bmatrix}\n",
    "\\begin{bmatrix}\\frac{1}{\\sigma_x^2 +\\sigma_z^2}\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\\sigma^2_x/(\\sigma_x^2 +\\sigma_z^2)  \\\\ \\sigma_{xv}/(\\sigma_x^2 +\\sigma_z^2)\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In other words, the Kalman gain for $x$ is \n",
    "\n",
    "$$K_x = \\frac{VAR(x)}{VAR(x)+VAR(z)}$$\n",
    "\n",
    "This should be very familiar to you from the univariate case. \n",
    "\n",
    "The Kalman gain for the velocity $\\dot x$ is\n",
    "$$K_{\\dot x} = \\frac{COV(x, \\dot x)}{VAR(x)+VAR(z)}$$\n",
    "\n",
    "What is the effect of this? Recall that we compute the state as \n",
    "\n",
    "$$\\begin{aligned}\\mathbf x \n",
    "&=\\mathbf{\\bar x}+\\mathbf K(z-\\mathbf{Hx)} \\\\\n",
    "&= \\mathbf{\\bar x}+\\mathbf Ky\\end{aligned}$$\n",
    "\n",
    "Here the residual $y$ is a scalar. Therefore it is multiplied into each element of $\\mathbf K$. Therefore we have\n",
    "\n",
    "$$\\begin{bmatrix}x \\\\ \\dot x\\end{bmatrix}=\\begin{bmatrix}\\bar x \\\\ \\bar{\\dot x}\\end{bmatrix} + \\begin{bmatrix}K_x \\\\ K_{\\dot x}\\end{bmatrix}y$$\n",
    "\n",
    "Which gives this system of equations: \n",
    "\n",
    "$$\\begin{aligned}x& = \\bar x + yK_x\\\\\n",
    "\\dot x &= \\bar{\\dot x} + yK_{\\dot x}\\end{aligned}$$\n",
    "\n",
    "The prediction $\\bar x$ was computed as $x + \\bar x \\Delta t$. If the prediction was perfect then the residual will be $y=0$ (ignoring noise in the measurement) and the velocity estimate will be unchanged. On the other hand, if the velocity estimate was very bad then the prediction will be very bad, and the residual will be large: $y >> 0$. In this case we update the velocity estimate with $yK_{\\dot x}$. $K_{\\dot x}$ is proportional to $COV(x,\\dot x)$. Therefore the velocity is updated by the error in the position times the value proportional to the covariance between the position and velocity. The higher the correlation the larger the correction. \n",
    "\n",
    "To bring this full circle, $COV(x,\\dot x)$ are the off-diagonal elements of $\\mathbf P$. Recall that those values were computed with $\\mathbf{FPF}^\\mathsf T$. So the covariance of position and velocity is computed during the predict step. The Kalman gain for the velocity is proportional to this covariance, and we adjust the velocity estimate based on how inaccurate it was during the last epoch times a value proportional to this covariance. \n",
    "\n",
    "In summary, these linear algebra equations may be unfamiliar to you, but computation is actually very simple. It is essentially the same computation that we performed in the g-h filter. Our constants are different in this chapter because we take the noise in the process model and sensors into account, but the math is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Filter\n",
    "\n",
    "Let's start varying our parameters to see the effect of various changes. This is a very normal thing to be doing with Kalman filters. It is difficult, and often impossible to exactly model our sensors. An imperfect model means imperfect output from our filter. Engineers spend a lot of time tuning Kalman filters so that they perform well with real world sensors. We will spend time now to learn the effect of these changes. As you learn the effect of each change you will develop an intuition for how to design a Kalman filter. Designing a Kalman filter is as much art as science. We are modeling a physical system using math, and models are imperfect.\n",
    "\n",
    "Let's look at the effects of the measurement noise $\\mathbf R$ and process noise $\\mathbf Q$. We will want to see the effect of different settings for $\\mathbf R$ and $\\mathbf Q$, so I have given the measurements a variance of 225 meters squared. That is very large, but it magnifies the effects of various design choices on the graphs, making it easier to recognize what is happening. Our first experiment holds $\\mathbf R$ constant while varying $\\mathbf Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(2)\n",
    "trk, zs = compute_dog_data(z_var=225, process_var=.02, count=50)\n",
    "\n",
    "run(track=trk, zs=zs, R=225, Q=200, P=P, plot_P=False, \n",
    "    title='R_var = 225 $m^2$, Q_var = 20 $m^2$')\n",
    "run(track=trk, zs=zs, R=225, Q=.02, P=P, plot_P=False, \n",
    "    title='R_var = 225 $m^2$, Q_var = 0.02 $m^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter in the first plot should follow the noisy measurement closely. In the second plot the filter should vary from the measurement quite a bit, and be much closer to a straight line than in the first graph. Why does ${\\mathbf Q}$ affect the plots this way?\n",
    "\n",
    "Let's remind ourselves of what the term *process uncertainty* means. Consider the problem of tracking a ball. We can accurately model its behavior in a vacuum with math, but with wind, varying air density, temperature, and a spinning ball with an imperfect surface our model will diverge from reality. \n",
    "\n",
    "In the first case we set `Q_var=20 m^2`, which is quite large. In physical terms this is telling the filter \"I don't trust my motion prediction step\" as we are saying that the variance in the velocity is 20. Strictly speaking, we are telling the filter there is a lot of external noise that we are not modeling with $\\small{\\mathbf F}$, but the upshot of that is to not trust the motion prediction step. The filter will be computing velocity ($\\dot x$), but then mostly ignoring it because we are telling the filter that the computation is extremely suspect. Therefore the filter has nothing to trust but the measurements, and thus it follows the measurements closely. \n",
    "\n",
    "In the second case we set `Q_var=0.02 m^2`, which is quite small. In physical terms we are telling the filter \"trust the prediction, it is really good!\". More strictly this actually says there is very small amounts of process noise (variance 0.02 $m^2$), so the process model is very accurate. So the filter ends up ignoring some of the measurement as it jumps up and down, because the variation in the measurement does not match our trustworthy velocity prediction.\n",
    "\n",
    "Now let's set `Q_var` to $0.2\\, m^2$, and bump `R_var` up to $10,000\\, m^2$. This is telling the filter that the measurement noise is very large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(track=trk, zs=zs, R=10000, Q=.2, P=P, plot_P=False, \n",
    "    title='R=$10,000\\, m^2$, Q=$.2\\, m^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of this can be subtle. We have created an suboptimal filter because the actual measurement noise variance is 225 $m^2$, not 10,000 $m^2$. By setting the filter's noise variance so high we force the filter to favor the prediction over the measurement. This can lead to apparently very smooth and good looking results. In the chart above the track may look extremely good to you since it follows the ideal path very closely. But, the 'great' behavior at the start should give you pause - the filter has not converged yet ($\\mathbf P$ is still large) so it should not be able to be so close to the actual position. We can see that $\\mathbf P$ has not converged because the entire chart is colored with the yellow background denoting the size of $\\mathbf P$. Let's see the result of a bad initial guess for the position by guessing the initial position to be 50 m and the initial velocity to be 1 m/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(track=trk, zs=zs, R=10000, Q=.2, P=P, plot_P=False,\n",
    "    x0=np.array([50., 1.]), \n",
    "    title='R=$10,000\\, m^2$, Q=$.2\\, m^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the filter cannot acquire the track. This happens because even though the filter is getting reasonably good measurements it assumes that the measurements are bad, and eventually predicts forward from a bad position at each step. If you think that perhaps that bad initial position would give similar results for a smaller measurement noise, let's set it back to the correct value of 225 $m^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(track=trk, zs=zs, R=225, Q=.2, P=P, plot_P=False, \n",
    "    x0=np.array([20., 1.]),\n",
    "    title='R=$225\\, m^2$, Q=$.2\\, m^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the filter initially struggles for several iterations to acquire the track, but then it accurately tracks our dog. In fact, this is nearly optimum - we have not designed $\\mathbf Q$ optimally, but $\\mathbf R$ is optimal. A rule of thumb for $\\mathbf Q$ is to set it between $\\frac{1}{2}\\Delta a$ to $\\Delta a$, where $\\Delta a$ is the maximum amount that the acceleration will change between sample periods. This only applies for the assumption we are making in this chapter - that acceleration is constant and uncorrelated between each time period. In the Kalman Math chapter we will discuss several different ways of designing $\\mathbf Q$.\n",
    "\n",
    "To some extent you can get similar looking output by varying either ${\\mathbf R}$ or ${\\mathbf Q}$, but I urge you to not 'magically' alter these until you get output that you like. Always think about the physical implications of these assignments, and vary ${\\mathbf R}$ and/or ${\\mathbf Q}$ based on your knowledge of the system you are filtering. Back that up with extensive simulations and/or trial runs of real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Detailed Examination of the Covariance Matrix\n",
    "\n",
    "Let's start by revisiting plotting a track. I've hard coded the data and noise in `zs_var_275` to avoid being at the mercy of the random number generator, which might generate data that does not illustrate what I want to talk about. I will start with `P=500`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kf_book.mkf_internal as mkf_internal\n",
    "\n",
    "var = 27.5\n",
    "data = mkf_internal.zs_var_275()\n",
    "run(track=trk, zs=zs, R=var, Q=.02, P=500., plot_P=True, \n",
    "    title='$P=500\\, m^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output we see a very large spike in the filter output at the beginning. We set $\\text{P}=500\\, \\mathbf{I}_2$ (this is shorthand notation for a 2x2 diagonal matrix with 500 in the diagonal). We now have enough information to understand what this means, and how the Kalman filter treats it. The 500 in the upper left hand corner corresponds to $\\sigma^2_x$; therefore we are saying the standard deviation of `x` is $\\sqrt{500}$, or roughly 22.36 m. Roughly 99% of the samples occur withing $3\\sigma$, therefore $\\sigma^2_x=500$ is telling the Kalman filter that the prediction (the prior) could be up to 67 meters off. That is a large error, so when the measurement spikes the Kalman filter distrusts its own estimate and jumps wildly to try to incorporate the measurement. Then, as the filter evolves $\\mathbf P$ quickly converges to a more realistic value.\n",
    "\n",
    "Let's look at the math behind this. The equation for the Kalman gain is\n",
    "\n",
    "$$\\mathbf K = \\mathbf{\\bar P} \\mathbf H^\\mathsf T\\mathbf{S}^{-1} \\approx \\frac{\\mathbf{\\bar P}\\mathbf H^\\mathsf T}{\\mathbf{S}} \n",
    "\\approx \\frac{\\mathsf{uncertainty}_\\mathsf{prediction}}{\\mathsf{uncertainty}_\\mathsf{measurement}}\\mathbf H^\\mathsf T\n",
    "$$\n",
    "\n",
    "It is a ratio of the uncertainty of the prediction vs measurement. Here the uncertainty in the prediction is large, so $\\mathbf K$ is large (near 1 if this was a scalar). $\\mathbf K$ is multiplied by the residual $\\textbf{y} = \\mathbf z - \\mathbf{H \\bar x}$, which is the measurement minus the prediction, so a large $\\mathbf K$ favors the measurement. Therefore if $\\mathbf P$ is large relative to the sensor uncertainty $\\mathbf R$ the filter will form most of the estimate from the measurement. \n",
    "\n",
    "\n",
    "Now let us see the effect of a smaller initial value of $\\mathbf P = 1.0\\, \\mathbf{I}_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(track=trk, zs=zs, R=var, Q=.02, P=1., plot_P=True, \n",
    "    title='$P=1\\, m^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *looks* good at first blush. The plot does not have the spike that the former plot did; the filter starts tracking the measurements and doesn't take any time to settle to the signal. However, if we look at the plots for P you can see that there is an initial spike for the variance in position, and that it never really converges. Poor design leads to a long convergence time, and suboptimal results. \n",
    "\n",
    "So despite the filter tracking very close to the actual signal we cannot conclude that the 'magic' is to use a small $\\mathbf P$. Yes, this will avoid having the Kalman filter take time to accurately track the signal, but if we are truly uncertain about the initial measurements this can cause the filter to generate very bad results. If we are tracking a living object we are probably very uncertain about where it is before we start tracking it. On the other hand, if we are filtering the output of a thermometer, we are as certain about the first measurement as the 1000th. For your Kalman filter to perform well you must set $\\mathbf P$ to a value that truly reflects your knowledge about the data. \n",
    "\n",
    "Let's see the result of a bad initial estimate coupled with a very small $\\mathbf P$. We will set our initial estimate at x = 100 m (whereas the dog actually starts at 0m), but set `P=1` m$^2$. This is clearly an incorrect value for $\\mathbf P$ as the estimate is off by 100 m but we tell the filter that it the $3\\sigma$ error is 3 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([100., 0.])\n",
    "run(track=trk, zs=zs, R=var, Q=.02, P=1., x0=x,\n",
    "    plot_P=False, title='$P=1\\, m^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the initial estimates are terrible and that it takes the filter a long time to start converging onto the signal . This is because we told the Kalman filter that we strongly believe in our initial estimate of 100 m and were incorrect in that belief.\n",
    "\n",
    "Now, let's provide a more reasonable value for `P` and see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([100., 0.])\n",
    "run(track=trk, zs=zs, R=var, Q=.02, P=500., x0=x,\n",
    "   plot_P=False, title='$P=500\\, m^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the Kalman filter is very uncertain about the initial state, so it converges onto the signal much faster. It is producing good output after only 5 to 6 epochs. With the theory we have developed so far this is about as good as we can do. However, this scenario is a bit artificial; if we do not know where the object is when we start tracking we do not initialize the filter to some arbitrary value, such as 0 m or 100 m. I address this in the **Filter Initialization** section below.\n",
    "\n",
    "Let's do another Kalman filter for our dog, and this time plot the covariance ellipses on the same plot as the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kf_book.mkf_internal import plot_track_ellipses\n",
    "\n",
    "def plot_covariances(count, R, Q=0, P=20., title=''):    \n",
    "    track, zs = compute_dog_data(R, Q, count)\n",
    "    f = pos_vel_filter(x=(0., 0.), R=R, Q=Q, P=P)\n",
    "\n",
    "    xs, cov = [], []\n",
    "    for z in zs:\n",
    "        f.predict()\n",
    "        f.update(z)\n",
    "\n",
    "        xs.append(f.x[0])\n",
    "        cov.append(f.P)\n",
    "    plot_track_ellipses(count, zs, xs, cov, title)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(121)\n",
    "plot_covariances(R=5, Q=.02, count=20, title='$R = 5\\, m^2$')\n",
    "plt.subplot(122)\n",
    "plot_covariances(R=.1, Q=.02, count=20, title='$R = 0.5\\, m^2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are viewing this in Jupyter Notebook or on the web, here is an animation of the filter filtering the data. I've tuned the filter parameters such that it is easy to see a change in $\\mathbf P$ as the filter progresses.\n",
    "<img src='animations/multivariate_track1.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output on these is a bit messy, but you should be able to see what is happening. In both plots we are drawing the covariance matrix for each point. We start with the covariance $\\mathbf P=(\\begin{smallmatrix}20&0\\\\0&20\\end{smallmatrix})$, which signifies a lot of uncertainty about our initial belief. After we receive the first measurement the Kalman filter updates this belief, and so the variance is no longer as large. In the top plot the first ellipse (the one on the far left) should be a slightly squashed ellipse. As the filter continues processing the measurements the covariance ellipse quickly shifts shape until it settles down to being a long, narrow ellipse tilted in the direction of movement.\n",
    "\n",
    "Think about what this means physically. The x-axis of the ellipse denotes our uncertainty in position, and the y-axis our uncertainty in velocity. So, an ellipse that is taller than it is wide signifies that we are more uncertain about the velocity than the position. Conversely, a wide, narrow ellipse shows high uncertainty in position and low uncertainty in velocity. Finally, the amount of tilt shows the amount of correlation between the two variables. \n",
    "\n",
    "The first plot, with $R=5 m^2$, finishes up with an ellipse that is wider than it is tall. If that is not clear I have printed out the variances for the last ellipse in the lower right hand corner.\n",
    "\n",
    "In contrast, the second plot, with `R=0.5` $m^2$, has a final ellipse that is taller than wide. The ellipses in the second plot are all much smaller than the ellipses in the first plot. This stands to reason because a small $\\small\\mathbf R$ implies a small amount of noise in our measurements. Small noise means accurate predictions, and thus a strong belief in our position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Explain Ellipse Differences\n",
    "\n",
    "Why are the ellipses for $\\mathbf R=5 m^2$ more tilted towards the horizontal than the ellipses for $\\mathbf R=0.5 m^2$. Hint: think about this in the context of what these ellipses mean physically, not in terms of the math. If you aren't sure about the answer,change $\\mathbf R$ to truly large and small numbers such as 100 $m^2$ and 0.1 $m^2$, observe the changes, and think about what this means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The x-axis is for position, and y-axis is velocity. An ellipse that is vertical, or nearly so, says there is no correlation between position and velocity, and an ellipse that is diagonal says that there is a lot of correlation. Phrased that way, the results sound unlikely. The tilt of the ellipse changes, but the correlation shouldn't be changing over time. But this is a measure of the *output of the filter*, not a description of the actual, physical world. When $\\mathbf R$ is very large we are telling the filter that there is a lot of noise in the measurements. In that case the Kalman gain $\\mathbf K$ is set to favor the prediction over the measurement, and the prediction comes from the velocity state variable. Thus there is a large correlation between $x$ and $\\dot x$. Conversely, if $\\mathbf R$ is small, we are telling the filter that the measurement is very trustworthy, and $\\mathbf K$ is set to favor the measurement over the prediction. Why would the filter want to use the prediction if the measurement is nearly perfect? If the filter is not using much from the prediction there will be very little correlation reported. \n",
    "\n",
    "**This is a critical point to understand!**. The Kalman filter is a mathematical model for a real world system. A report of little correlation *does not mean* there is no correlation in the physical system, just that there was no *linear* correlation in the mathematical model. It's a report of how much measurement vs prediction was incorporated into the model.  \n",
    "\n",
    "Let's bring that point home with a truly large measurement error. We will set $\\mathbf R=200\\, m^2$. Think about what the plot will look like before looking at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_covariances(R=200., Q=.2, count=5, title='$R = 200\\, m^2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope the result was what you were expecting. The ellipse quickly became very wide and not very tall. It did this because the Kalman filter mostly used the prediction vs the measurement to produce the filtered result. We can also see how the filter output is slow to acquire the track. The Kalman filter assumes that the measurements are extremely noisy, and so it is very slow to update its estimate for $\\dot x$. \n",
    "\n",
    "Keep looking at these plots until you grasp how to interpret the covariance matrix $\\mathbf P$. When you work with a $9{\\times}9$ matrix it may seem overwhelming - there are 81 numbers to interpret. Just break it down - the diagonal contains the variance for each state variable, and all off diagonal elements are the product of two variances and a scaling factor $p$. You cannot plot a $9{\\times}9$ matrix on the screen so you have to develop your intuition and understanding in this simple, 2-D case. \n",
    "\n",
    ">When plotting covariance ellipses, make sure to always use ax.set_aspect('equal') or plt.axis('equal') in your code (the former lets you set the xlim and ylim values). If the axis use different scales the ellipses will be drawn distorted. For example, the ellipse may be drawn as being taller than it is wide, but it may actually be wider than tall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Initialization\n",
    "\n",
    "\n",
    "There are many schemes for initializing the filter. The following approach performs well in most situations. In this scheme you do not initialize the filter until you get the first measurement, $\\mathbf z_0$. From this you can compute the initial value for $\\mathbf x$ with $\\mathbf x_0 = \\mathbf z_0$. If  $\\mathbf z$ is not of the same size, type, and units as $\\mathbf x$, which is usually the case, we can use our measurement function as follow.\n",
    "\n",
    "We know\n",
    "\n",
    "$$\\mathbf z = \\mathbf{Hx}$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf H^{-1}\\mathbf{Hx} &= \\mathbf H^{-1}\\mathbf z \\\\\n",
    "\\mathbf x &= \\mathbf H^{-1}\\mathbf z\\end{aligned}$$\n",
    "\n",
    "Matrix inversion requires a square matrix, but $\\mathbf H$ is rarely square. SciPy will compute the Moore-Penrose pseudo-inverse of a matrix with `scipy.linalg.pinv`, so your code might look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import pinv\n",
    "\n",
    "H = np.array([[1, 0.]]) \n",
    "z0 = 3.2\n",
    "x = np.dot(pinv(H), z0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specialized knowledge of your problem domain may lead you to a different computation, but this is one way to do it. For example, if the state includes velocity you might take the first two measurements of position, compute the difference, and use that as the initial velocity.\n",
    "\n",
    "Now we need to compute a value for $\\mathbf P$. This will vary by problem, but in general you will use the measurement error $\\mathbf R$ for identical terms, and maximum values for the rest of the terms. Maybe that isn't clear. In this chapter we have been tracking and object using position and velocity as the state, and the measurements have been positions. In that case we would initialize $\\mathbf P$ with\n",
    "\n",
    "$$\\mathbf P = \\begin{bmatrix}\\mathbf R_0 & 0 \\\\0 & vel_{max}^2\\end{bmatrix}$$\n",
    "\n",
    "The diagonal of $\\mathbf P$ contains the variance of each state variable, so we populate it with reasonable values. $\\mathbf R$ is a reasonable variance for the position, and the maximum velocity squared is a reasonable variance for the velocity. It is squared because variance is squared: $\\sigma^2$.\n",
    "\n",
    "You really need to understand the domain in which you are working and initialize your filter on the best available information. For example, suppose we were trying to track horses in a horse race. The initial measurements might be very bad, and provide you with a position far from the starting gate. We know that the horse must start at the starting gate; initializing the filter to the initial measurement would lead to suboptimal results. In this scenario we would want to always initialize the Kalman filter with the starting gate position of the horse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "The Kalman filter is designed as a recursive algorithm - as new measurements come in we immediately create a new estimate. But it is very common to have a set of data that have been already collected which we want to filter. Kalman filters can be run in a batch mode, where all of the measurements are filtered at once. We have implemented this in `KalmanFilter.batch_filter()`. Internally, all the function does is loop over the measurements and collect the resulting state and covariance estimates in arrays. It simplifies your logic and conveniently gathers all of the outputs into arrays. I often use this function, but waited until the end of the chapter so you would become very familiar with the predict/update cyle that you must run.\n",
    "\n",
    "First collect your measurements into an array or list. Maybe it is in a CSV file:\n",
    "\n",
    "```python\n",
    "zs = read_altitude_from_csv('altitude_data.csv')\n",
    "```\n",
    "\n",
    "Or maybe you will generate it using a generator:\n",
    "\n",
    "```python\n",
    "zs = [some_func(i) for i in range(1000)]\n",
    "```\n",
    "\n",
    "Then call the `batch_filter()` method.\n",
    "\n",
    "```python\n",
    "Xs, Ps, Xs_prior, Ps_prior = kfilter.batch_filter(zs)\n",
    "```\n",
    "\n",
    "The function takes the list of measurements, filters it, and returns an  NumPy array of state estimates (Xs), covariance matrices (Ps), and the priors for the same (Xs_prior, Ps_prior).\n",
    "\n",
    "Here is a complete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 50\n",
    "track, zs = compute_dog_data(10, .2, count)\n",
    "P = np.diag([500., 49.])\n",
    "f = pos_vel_filter(x=(0., 0.), R=3., Q=.02, P=P)\n",
    "xs, _, _, _ = f.batch_filter(zs)\n",
    "\n",
    "book_plots.plot_measurements(range(1, count + 1), zs)\n",
    "book_plots.plot_filter(range(1, count + 1), xs[:, 0])\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch filter takes an optional `filterpy.common.Saver` object. If provided, all of the filter's attributes will be saved as well. This is useful if you want to inspect values other than the state and covariance. Here I plot the residual to see if it appears like noise centered around 0. This is a quick visual inspection to see if the filter is well designed. If if drifts from zero, or doesn't look like noise, the filter is poorly designed and/or the processes are not Gaussian. We will discuss this in detail in later chapters. For now consider this a demonstration of the `Saver` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track, zs = compute_dog_data(10, .2, 200)\n",
    "P = np.diag([500., 49.])\n",
    "f = pos_vel_filter(x=(0., 0.), R=3., Q=.02, P=P)\n",
    "s = Saver(f)\n",
    "xs, _, _, _ = f.batch_filter(zs, saver=s)\n",
    "s.to_array()\n",
    "plt.plot(s.y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing the Results\n",
    "\n",
    "This book includes a chapter on smoothing the results; I will not repeat the information here. However, it is so easy to use, and offers such a profoundly improved output that I will tease you with a few examples. The smoothing chapter is not especially difficult; you are sufficiently prepared to read it now.\n",
    "\n",
    "Let's assume that we are tracking a car that has been traveling in a straight line. We get a measurement that implies that the car is starting to turn to the left. The Kalman filter moves the state estimate somewhat towards the measurement, but it cannot judge whether this is a particularly noisy measurement or the true start of a turn. \n",
    "\n",
    "However, if we have future measurements we can decide if a turn was made. Suppose the subsequent measurements all continue turning left. We can then be sure that that a turn was initiated. On the other hand, if the subsequent measurements continued on in a straight line we would know that the measurement was noisy and should be mostly ignored. Instead of making an estimate part way between the measurement and prediction the estimate will either fully incorporate the measurement or ignore it, depending on what the future measurements imply about the object's movement.\n",
    "\n",
    "`KalmanFilter` implements a form of this algorithm which is called an *RTS smoother*, named after the inventors of the algorithm: Rauch, Tung, and Striebel. The method is `rts_smoother()`. To use it pass in the means and covariances computed from the `batch_filter` step, and receive back the smoothed means, covariances, and Kalman gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "count = 50\n",
    "seed(8923)\n",
    "\n",
    "P = np.diag([500., 49.])\n",
    "f = pos_vel_filter(x=(0., 0.), R=3., Q=.02, P=P)\n",
    "track, zs = compute_dog_data(3., .02, count)\n",
    "Xs, Covs, _, _ = f.batch_filter(zs)\n",
    "Ms, Ps, _, _ = f.rts_smoother(Xs, Covs)\n",
    "\n",
    "book_plots.plot_measurements(zs)\n",
    "plt.plot(Xs[:, 0], ls='--', label='Kalman Position')\n",
    "plt.plot(Ms[:, 0], label='RTS Position')\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is fantastic! Two things are very apparent to me in this chart. First, the RTS smoother's output is much smoother than the KF output. Second, it is almost always more accurate than the KF output (we will examine this claim in detail in the **Smoothing** chapter). The improvement in the velocity, which is a hidden variable, is even more dramatic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xs[:, 1], ls='--', label='Kalman Velocity')\n",
    "plt.plot(Ms[:, 1], label='RTS Velocity')\n",
    "plt.legend(loc=4)\n",
    "plt.gca().axhline(1, lw=1, c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore why this is so in the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Compare Velocities\n",
    "\n",
    "Since we are plotting velocities let's look at what the 'raw' velocity is, which we can compute by subtracting subsequent measurements. i.e the velocity at time 1 can be approximated by `xs[1] - xs[0]`. Plot the raw value against the values estimated by the Kalman filter and by the RTS filter. Discuss what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = np.diff(Xs[:, 0], axis=0)\n",
    "plt.scatter(range(1, len(dx) + 1), dx, facecolor='none', \n",
    "            edgecolor='k', lw=2, label='Raw velocity')\n",
    "plt.plot(Xs[:, 1], ls='--', label='Filter')\n",
    "plt.plot(Ms[:, 1], label='RTS')\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the noise swamps the signal, causing the raw values to be essentially worthless. The filter is maintaining a separate estimate for velocity. The Kalman gain $\\mathbf K$ is multidimensional. For example, it might have the value  $\\mathbf K = [0.1274, 0.843]^\\mathsf T$. the first value is used to scale the residual of the position, and the second value will scale the residual of the velocity. The covariance matrix tells the filter how correlated the position and velocity are, and each will be optimally filtered. \n",
    "\n",
    "I show this to reiterate the importance of using Kalman filters to compute velocities, accelerations, and even higher order values. I use a Kalman filter even when my measurements are so accurate that I am willing to use them unfiltered because it allows me accurate estimates for velocities and accelerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Summary\n",
    "\n",
    "Multivariate Gaussians allow us to simultaneously handle multiple dimensions, both spacial and others (velocity, etc). We made a key insight: hidden variables have the ability to significantly increase the accuracy of the filter. This is possible because the hidden variables are correlated with the observed variables. \n",
    "\n",
    "I gave an intuitive definition of *observability*. Observability was invented by Dr. Kalman for linear systems, and there is a fair amount of theory behind it. It answers the question of whether a system state can be determined by observing the system's output. For our problems this has been easy to determine, but more complicated systems may require rigorous analysis. Wikipedia's [Observability](https://en.wikipedia.org/wiki/Observability) article has an overview; if you need to learn the topic [[Grewal2008]](#References) is a good source. \n",
    "\n",
    "There is one important caveat about hidden variables. It is easy to construct a filter that produces estimates for hidden variables. I could write a filter that estimates the color of a tracked car. But there is no way to compute car color from positions, so the estimate for the color will be nonsense. The designer must verify that these variables are being estimated correctly. If you do not have a velocity sensor and yet are estimating velocity, you will need to test that the velocity estimates are correct.; do not trust that they are. For example, suppose the velocity has a periodic component to it - it looks like a sine wave. If your sample time is less than 2 times the frequency you will not be able to accurately estimate the velocity (due to Nyquist's Theorem). Imagine that the sample period is equal to the frequency of the velocity. The filter will report that the velocity is constant because it samples the system at the same point on the sin wave. \n",
    "\n",
    "Initialization poses a particularly difficult problem for hidden variables. If you start with a bad initialization the filter can usually recover the observed variables, but may struggle and fail with the hidden one. Estimating hidden variables is a powerful tool, but a dangerous one. \n",
    "\n",
    "I established a series of steps for designing a Kalman filter. These are not a usual part of the Kalman filter literature, and are only meant as a guide, not a prescription. Designing for a hard problem is an iterative process. You make a guess at the state vector, work out what your measurement and state models are, run some tests, and then alter the design as necessary. \n",
    "\n",
    "The design of $\\mathbf R$ and $\\mathbf Q$ is often quite challenging. I've made it appear to be quite scientific. Your sensor has Gaussian noise of $\\mathcal{N}(0, \\sigma^2)$, so set $\\mathbf R=\\sigma^2$. Easy! This is a dirty lie. Sensors are not Gaussian. We started the book with a bathroom scale. Suppose $\\sigma=1$ kg, and you try to weigh something that weighs 0.5 kg. Theory tells us we will get negative measurements, but of course the scale will never report weights less than zero. Real world sensors typically have *fat tails* (known as *kurtosis*) and *skew*. In some cases, such as with the scale, one or both tails are truncated.\n",
    "\n",
    "The case with $\\mathbf Q$ is more dire. I hope you were skeptical when I blithely assigned a noise matrix to my prediction about the movements of a dog. Who can say what a dog will do next? The Kalman filter in my GPS doesn't know about hills, the outside winds, or my terrible driving skills. Yet the filter requires a precise number to encapsulate all of that information, and it needs to work while I drive off-road in the desert, and when a Formula One champion drives on a track.\n",
    "\n",
    "These problems led some researchers and engineers to derogatorily call the Kalman filter a 'ball of mud'. In other words, it doesn't always hold together so well. Another term to know - Kalman filters can become *smug*. Their estimates are based solely on what you tell it the noises are. Those values can lead to overly confident estimates. $\\mathbf P$ gets smaller and smaller while the filter is actually becoming more and more inaccurate! In the worst case the filter diverges. We will see a lot of that when we start studying nonlinear filters. \n",
    "\n",
    "The Kalman filter is a mathematical model of the world. The output is only as accurate as that model. To make the math tractable we had to make some assumptions. We assume that the sensors and motion model have Gaussian noise. We assume that everything is linear. If that is true, the Kalman filter is *optimal* in a least squares sense. This means that there is no way to make a better estimate than what the filter gives us. However, these assumption are almost never true, and hence the model is necessarily limited, and a working filter is rarely optimal.\n",
    "\n",
    "In later chapters we will deal with the problem of nonlinearity. For now I want you to understand that designing the matrices of a linear filter is an experimental procedure more than a mathematical one. Use math to establish the initial values, but then you need to experiment. If there is a lot of unaccounted noise in the world (wind, etc.) you may have to make $\\mathbf Q$ larger. If you make it too large the filter fails to respond quickly to changes. In the **Adaptive Filters** chapter you will learn some alternative techniques that allow you to change the filter design in real time in response to the inputs and performance, but for now you need to find one set of values that works for the conditions your filter will encounter. Noise matrices for an acrobatic plane might be different if the pilot is a student than if the pilot is an expert as the dynamics will be quite different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <A name=\"[wiki_article]\">[1]</A> 'Kalman Filters'. Wikipedia\n",
    "https://en.wikipedia.org/wiki/Kalman_filter#Details\n",
    "\n",
    "* **[Grewal2008]** Grewal, Mohinder S., Andrews, Angus P. *Kalman Filtering: Theory and Practice Using MATLAB*. Third Edition. John Wiley & Sons. 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01f6a703d143467eb6a8756e8da1efcb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": false,
       "description": "F10",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_ffd71501a24344a4be28b35a6c24096c",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.1,
       "style": "IPY_MODEL_b99e133427774af5bb781c533bf95f7b",
       "value": 0
      }
     },
     "1c5b4923ffb542f99444e0ff8c0b6aaf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a16f5c5199d495593c2b8805778b5ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "IntSliderView",
       "continuous_update": false,
       "description": "F00",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_4d4f9df432184648964655fd003171d8",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": "d",
       "step": 1,
       "style": "IPY_MODEL_c2f62091b1384633835bf7dab290e612",
       "value": 1
      }
     },
     "3c8580d064144f959f578481a3580b60": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_add288f45fd5486c9c787510e3352842",
       "msg_id": "",
       "outputs": []
      }
     },
     "43283d93f77c46419ee80addf6c97262": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d4f9df432184648964655fd003171d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d59516371b94d8cb365cf059916b203": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": false,
       "description": "covar",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_43283d93f77c46419ee80addf6c97262",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.1,
       "style": "IPY_MODEL_6af35bd98b4d40858858a10ed6237587",
       "value": 0
      }
     },
     "6af35bd98b4d40858858a10ed6237587": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "6e4b07b034384ac6bb6872238768aa3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73e2335edce9462eacea4c19c8f94fe1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3a16f5c5199d495593c2b8805778b5ed",
        "IPY_MODEL_fc7865559c15404bb1f5ee9d048609f4",
        "IPY_MODEL_01f6a703d143467eb6a8756e8da1efcb",
        "IPY_MODEL_ef06a5a66be34c6bb8b98d9c3374b869",
        "IPY_MODEL_4d59516371b94d8cb365cf059916b203",
        "IPY_MODEL_3c8580d064144f959f578481a3580b60"
       ],
       "layout": "IPY_MODEL_6e4b07b034384ac6bb6872238768aa3d"
      }
     },
     "add288f45fd5486c9c787510e3352842": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b99e133427774af5bb781c533bf95f7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "bdcf86db1c254a5697cc1e700d7dbdad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "c2f62091b1384633835bf7dab290e612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "cc63ade828f942ea9906f6a15053cfc8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db532fd7630a4411942bfbd67111d962": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "ef06a5a66be34c6bb8b98d9c3374b869": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": false,
       "description": "F11",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_1c5b4923ffb542f99444e0ff8c0b6aaf",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.1,
       "style": "IPY_MODEL_db532fd7630a4411942bfbd67111d962",
       "value": 1
      }
     },
     "fc7865559c15404bb1f5ee9d048609f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": false,
       "description": "F01(dt)",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_cc63ade828f942ea9906f6a15053cfc8",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.1,
       "style": "IPY_MODEL_bdcf86db1c254a5697cc1e700d7dbdad",
       "value": 1
      }
     },
     "ffd71501a24344a4be28b35a6c24096c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
